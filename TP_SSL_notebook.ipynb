{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OI94rsx_Efro"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckurtz/teaching_UPC_UFRMI/blob/main/selfsupervised_demos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eBTL5OZDcX6"
      },
      "source": [
        "# References\n",
        "\n",
        "[Course Webpage](https://sites.google.com/view/berkeley-cs294-158-sp20/home)\n",
        "\n",
        "[1] Pathak, Deepak, et al. \"Context encoders: Feature learning by inpainting.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n",
        "\n",
        "[2] Gidaris, Spyros, Praveer Singh, and Nikos Komodakis. \"Unsupervised representation learning by predicting image rotations.\" arXiv preprint arXiv:1803.07728 (2018).\n",
        "\n",
        "[3] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" arXiv preprint arXiv:2002.05709 (2020).\n",
        "\n",
        "[4] Noroozi, Mehdi, and Paolo Favaro. \"Unsupervised learning of visual representations by solving jigsaw puzzles.\" European Conference on Computer Vision. Springer, Cham, 2016.\n",
        "\n",
        "[5] Wang, Xiaolong, Allan Jabri, and Alexei A. Efros. \"Learning correspondence from the cycle-consistency of time.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n",
        "\n",
        "[6] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. \"Representation learning with contrastive predictive coding.\" arXiv preprint arXiv:1807.03748 (2018).\n",
        "\n",
        "[7] Hénaff, Olivier J., et al. \"Data-efficient image recognition with contrastive predictive coding.\" arXiv preprint arXiv:1905.09272 (2019).\n",
        "\n",
        "[8] Tian, Yonglong, Dilip Krishnan, and Phillip Isola. \"Contrastive multiview coding.\" arXiv preprint arXiv:1906.05849 (2019).\n",
        "\n",
        "[9] He, Kaiming, et al. \"Momentum contrast for unsupervised visual representation learning.\" arXiv preprint arXiv:1911.05722 (2019).\n",
        "\n",
        "[10] Doersch, Carl, Abhinav Gupta, and Alexei A. Efros. \"Unsupervised visual representation learning by context prediction.\" Proceedings of the IEEE International Conference on Computer Vision. 2015.\n",
        "\n",
        "[11] Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhDFwV3dDsY5"
      },
      "source": [
        "# Getting Started\n",
        "Go to **Runtime -> Change runtime type** and make sure **Hardward accelerator** is set to **GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR9QSsyEvY_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a09718-c5a0-4d46-83fa-7025732f540d"
      },
      "source": [
        "!if [ -d cs294-158-ssl ]; then rm -Rf cs294-158-ssl; fi\n",
        "!git clone https://github.com/FrenchFroonz/cs294-158-ssl\n",
        "!pip install cs294-158-ssl/\n",
        "\n",
        "import os\n",
        "os.chdir('cs294-158-ssl')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cs294-158-ssl'...\n",
            "remote: Enumerating objects: 641, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 641 (delta 108), reused 90 (delta 90), pack-reused 517 (from 1)\u001b[K\n",
            "Receiving objects: 100% (641/641), 3.11 MiB | 51.39 MiB/s, done.\n",
            "Resolving deltas: 100% (458/458), done.\n",
            "Processing ./cs294-158-ssl\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: deepul_helper\n",
            "  Building wheel for deepul_helper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepul_helper: filename=deepul_helper-0.1.0-py3-none-any.whl size=28986 sha256=c3aa7f019c741169a949e5601012103379fc049ca2edd6741ae5a3d3133eb994\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/df/de/2522718f33fc5fb5d513646711c5929b728d06ba7f1c0aaaae\n",
            "Successfully built deepul_helper\n",
            "Installing collected packages: deepul_helper\n",
            "  Attempting uninstall: deepul_helper\n",
            "    Found existing installation: deepul_helper 0.1.0\n",
            "    Uninstalling deepul_helper-0.1.0:\n",
            "      Successfully uninstalled deepul_helper-0.1.0\n",
            "Successfully installed deepul_helper-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrZxut8cQ_ps"
      },
      "source": [
        "Run the cells below to download the necessary pretrained models. It should take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJPX6WS5QWSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a9b64a-483f-47dd-be83-96e7809f1930"
      },
      "source": [
        "!wget https://camille-kurtz.com/teaching/data.zip\n",
        "!unzip -qq data.zip\n",
        "!rm data.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-09 17:35:23--  https://camille-kurtz.com/teaching/data.zip\n",
            "Resolving camille-kurtz.com (camille-kurtz.com)... 213.186.33.3\n",
            "Connecting to camille-kurtz.com (camille-kurtz.com)|213.186.33.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4227668031 (3.9G) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   3.94G  12.7MB/s    in 5m 17s  \n",
            "\n",
            "2025-11-09 17:40:42 (12.7 MB/s) - ‘data.zip’ saved [4227668031/4227668031]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRxGTJeSTSYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e72e6727-a653-4229-fb65-17e7273c954b"
      },
      "source": [
        "!wget https://camille-kurtz.com/teaching/results.zip\n",
        "!unzip -qq results.zip\n",
        "!rm results.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-09 17:41:46--  https://camille-kurtz.com/teaching/results.zip\n",
            "Resolving camille-kurtz.com (camille-kurtz.com)... 213.186.33.3\n",
            "Connecting to camille-kurtz.com (camille-kurtz.com)|213.186.33.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2636425194 (2.5G) [application/zip]\n",
            "Saving to: ‘results.zip’\n",
            "\n",
            "results.zip         100%[===================>]   2.46G  13.7MB/s    in 3m 40s  \n",
            "\n",
            "2025-11-09 17:45:28 (11.4 MB/s) - ‘results.zip’ saved [2636425194/2636425194]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjZjp6-fFVEJ"
      },
      "source": [
        "The models and demos shown were pre-trained. The code used for all the demos can be found in the github repo [here](https://github.com/wilson1yan/cs294-158-ssl). Follow the README to train models on CIFAR10 or ImageNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8BTJ0Mt5oVy"
      },
      "source": [
        "# Self-Supervised Learning Tasks\n",
        "Self-supervised learning is a rapidly growing field, its success largely accelerated by growing compute and the vast amount of unlabeled data available for training. The hope is that by pretraining on specially designed self-supervised tasks, the models would be able to learn semantically meaningful representations to be used for downstream tasks. In the following demos, we will look at a few examples of these self-supervised tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2knEnPv38kOW"
      },
      "source": [
        "from deepul_helper.demos import load_model_and_data, evaluate_accuracy, display_nearest_neighbors, show_context_encoder_inpainting\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8KalQiXyJE-"
      },
      "source": [
        "## Demo 1: Context Encoder [[1]](https://arxiv.org/abs/1604.07379)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRyHjNsSDzPY"
      },
      "source": [
        "The context encoder structures its self-supervised learning task by inpainting masked images. For example, the figure below shows different masking shapes, such as center masking, random block masking, and segmentation masking. Note that segmentation masking (c) is not purely self-supervised since we would need to train a image segmentation model which requires labels. However, the other two masking schemes (a) and (b) and purely self-supervised.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1fhzkULYTtyMGUUF2n9dlPayJSdcY5pRv)\n",
        "\n",
        "More formally, the context encoder optimizes the following reconstruction loss:\n",
        "$$\\mathcal{L}_{rec} = \\left\\Vert \\hat{M} \\odot (x - F((1 - \\hat{M})\\odot x)) \\right\\Vert^2_2$$\n",
        "where $\\hat{M}$ is the masked region, $x$ is the image, and $F$ is the context encoder that tries to reconstruct the masked portion. In addition to the reconstruction loss, the paper introduces an adversarial loss that encourages more realistic inpaintings.\n",
        "$$L_{adv} = \\max_D \\mathbb{E}_{x\\in \\chi} [\\log(D(x)) + \\log(1 - D(F((1-\\hat{M})\\odot x)))]$$\n",
        "However, this demo does not use the adversarial portion of the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li_KasuCjQhp"
      },
      "source": [
        "### Example Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC-sluHYjSVB"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ContextEncoder(nn.Module):\n",
        "    metrics = ['Loss']\n",
        "    metrics_fmt = [':.4e']\n",
        "\n",
        "    def __init__(self, dataset, n_classes):\n",
        "        super().__init__()\n",
        "        input_channels = 3\n",
        "\n",
        "        self.latent_dim = 4000\n",
        "\n",
        "        # Encodes the masked image\n",
        "        self.encoder = nn.Sequential(\n",
        "            # 128 x 128 Input\n",
        "            nn.Conv2d(input_channels, 64, 4, stride=2, padding=1), # 64 x 64\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 64, 4, stride=2, padding=1), # 32 x 32\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1), # 16 x 16\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 4, stride=2, padding=1), # 8 x 8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, 4, stride=2, padding=1), # 4 x 4\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, self.latent_dim, 4) # 1 x 1\n",
        "        )\n",
        "\n",
        "        # Only reconstructs the masked part of the image and not the whole image\n",
        "        self.decoder = nn.Sequential(\n",
        "           nn.BatchNorm2d(self.latent_dim),\n",
        "           nn.ReLU(inplace=True),\n",
        "           nn.ConvTranspose2d(self.latent_dim, 512, 4, stride=1, padding=0), # 4 x 4\n",
        "           nn.BatchNorm2d(512),\n",
        "           nn.ReLU(inplace=True),\n",
        "           nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1), # 8 x 8\n",
        "           nn.BatchNorm2d(256),\n",
        "           nn.ReLU(inplace=True),\n",
        "           nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1), # 16 x 16\n",
        "           nn.BatchNorm2d(128),\n",
        "           nn.ReLU(inplace=True),\n",
        "           nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), # 32 x 32\n",
        "           nn.BatchNorm2d(64),\n",
        "           nn.ReLU(inplace=True),\n",
        "           nn.ConvTranspose2d(64, input_channels, 4, stride=2, padding=1), # 64 x 64\n",
        "           nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def construct_classifier(self):\n",
        "        classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.BatchNorm1d(self.latent_dim, affine=False),\n",
        "            nn.Linear(self.latent_dim, self.n_classes)\n",
        "        )\n",
        "        return classifier\n",
        "\n",
        "    def forward(self, images):\n",
        "        # Extract a 64 x 64 center from 128 x 128 image\n",
        "        images_center = images[:, :, 32:32+64, 32:32+64].clone()\n",
        "        images_masked = images.clone()\n",
        "        # Mask out a 64 x 64 center with slight overlap\n",
        "        images_masked[:, 0, 32+4:32+64-4, 32+4:32+64-4] = 2 * 117.0/255.0 - 1.0\n",
        "        images_masked[:, 1, 32+4:32+64-4, 32+4:32+64-4] = 2 * 104.0/255.0 - 1.0\n",
        "        images_masked[:, 2, 32+4:32+64-4, 32+4:32+64-4] = 2 * 123.0/255.0 - 1.0\n",
        "\n",
        "        z = self.encoder(images_masked)\n",
        "        center_recon = self.decoder(z)\n",
        "\n",
        "        return dict(Loss=F.mse_loss(center_recon, images_center)), torch.flatten(z, 1)\n",
        "\n",
        "    def encode(self, images):\n",
        "        images_masked = images\n",
        "        images_masked[:, 0, 32+4:32+64-4, 32+4:32+64-4] = 2 * 117.0/255.0 - 1.0\n",
        "        images_masked[:, 1, 32+4:32+64-4, 32+4:32+64-4] = 2 * 104.0/255.0 - 1.0\n",
        "        images_masked[:, 2, 32+4:32+64-4, 32+4:32+64-4] = 2 * 123.0/255.0 - 1.0\n",
        "        return self.encoder(images_masked)\n",
        "\n",
        "    def reconstruct(self, images):\n",
        "        images_center = images[:, :, 32:32+64, 32:32+64].clone()\n",
        "        images_masked = images.clone()\n",
        "        images_masked[:, 0, 32+4:32+64 - 4, 32+4:32+64-4] = 2 * 117.0/255.0 - 1.0\n",
        "        images_masked[:, 1, 32+4:32+64 - 4, 32+4:32+64-4] = 2 * 104.0/255.0 - 1.0\n",
        "        images_masked[:, 2, 32+4:32+64 - 4, 32+4:32+64-4] = 2 * 123.0/255.0 - 1.0\n",
        "\n",
        "        z = self.encoder(images_masked)\n",
        "        center_recon = self.decoder(z)\n",
        "\n",
        "        images_recon = images_masked.clone()\n",
        "        images_recon[:, :, 32:32+64, 32:32+64] = center_recon\n",
        "        return images_masked, images_recon\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvVq8IbRwd9L"
      },
      "source": [
        "### Inpainting Examples\n",
        "For each pair of images, the left image is the masked input and the right the inpainted reconstruction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmsjo0U5EQjW"
      },
      "source": [
        "show_context_encoder_inpainting()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owIoT07hwl7b"
      },
      "source": [
        "### Linear Classification\n",
        "By design the model architecture is an encoder -> decoder module. We can use the bottleneck layer as our learned representation. Below, we show linear classification accuracy results on CIFAR10 using the learned representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW33IV4fw88x"
      },
      "source": [
        "model, linear_classifier, train_loader, test_loader = load_model_and_data('context_encoder')\n",
        "evaluate_accuracy(model, linear_classifier, train_loader, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiLZdyXLw9Qo"
      },
      "source": [
        "### Nearest Neighbors\n",
        "Another way to evaluate our learned representation is to look at nearest neighbors to random encoded images in latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAxqYGEbxI8H"
      },
      "source": [
        "display_nearest_neighbors('context_encoder', model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoTHjrBNzRyL"
      },
      "source": [
        "## Demo 2: Rotation Prediction [[2]](https://arxiv.org/abs/1803.07728)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cK8-rW0EQ8d"
      },
      "source": [
        "In this paper, the authors show that accurately predicting the degrees of rotation in images is a self-supervised learning task that learns good representations for downstream tasks.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1eHXLH-N_6uMGRzdf1Wjnga26qlS5-FRv)\n",
        "\n",
        "More specifically, the authors showed that training a common CNN architecture (AlexNet, ResNet) on the rotation task learns semantically interpretable convolutional masks similar to those learned in supervised learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dBi6z7ajin2"
      },
      "source": [
        "### Example Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shrEf5ZqjkCn"
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class RotationPrediction(nn.Module):\n",
        "    metrics = ['Loss', 'Acc1']\n",
        "    metrics_fmt = [':.4e', ':6.2f']\n",
        "\n",
        "    def __init__(self, dataset, n_classes):\n",
        "        super().__init__()\n",
        "        if dataset == 'cifar10':\n",
        "            self.model = NetworkInNetwork()\n",
        "            self.latent_dim = 192 * 8 * 8\n",
        "            self.feat_layer = 'conv2'\n",
        "        elif 'imagenet' in dataset:\n",
        "            self.model = AlexNet()\n",
        "            self.latent_dim = 256 * 13 * 13\n",
        "            self.feat_layer = 'conv5'\n",
        "        else:\n",
        "            raise Exception('Unsupported dataset:', dataset)\n",
        "        self.dataset = dataset\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def construct_classifier(self):\n",
        "        if self.dataset == 'cifar10':\n",
        "            classifier = nn.Sequential(\n",
        "                Flatten(),\n",
        "                nn.BatchNorm1d(self.latent_dim, affine=False),\n",
        "                nn.Linear(self.latent_dim, self.n_classes)\n",
        "            )\n",
        "        elif 'imagenet' in self.dataset:\n",
        "            classifier = nn.Sequential(\n",
        "                nn.AdaptiveMaxPool2d((6, 6)),\n",
        "                nn.BatchNorm2d(256, affine=False),\n",
        "                Flatten(),\n",
        "                nn.Linear(256 * 6 * 6, self.n_classes)\n",
        "            )\n",
        "        else:\n",
        "            raise Exception('Unsupported dataset:', dataset)\n",
        "        return classifier\n",
        "\n",
        "    def forward(self, images):\n",
        "        batch_size = images.shape[0]\n",
        "        images, targets = self._preprocess(images)\n",
        "        targets = targets.to(images.get_device())\n",
        "\n",
        "        logits, zs = self.model(images, out_feat_keys=('classifier', self.feat_layer))\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        pred = logits.argmax(dim=-1)\n",
        "        correct = pred.eq(targets).float().sum()\n",
        "        acc = correct / targets.shape[0] * 100.\n",
        "\n",
        "        return dict(Loss=loss, Acc1=acc), zs[:batch_size]\n",
        "\n",
        "    def encode(self, images):\n",
        "        zs = self.model(images, out_feat_keys=(self.feat_layer,))\n",
        "        return zs\n",
        "\n",
        "    def _preprocess(self, images):\n",
        "        batch_size = images.shape[0]\n",
        "        images_90 = torch.flip(images.transpose(2, 3), (2,))\n",
        "        images_180 = torch.flip(images, (2, 3))\n",
        "        images_270 = torch.flip(images, (2,)).transpose(2, 3)\n",
        "        images_batch = torch.cat((images, images_90, images_180, images_270), dim=0)\n",
        "        targets = torch.arange(4).long().repeat(batch_size)\n",
        "        targets = targets.view(batch_size, 4).transpose(0, 1)\n",
        "        targets = targets.contiguous().view(-1)\n",
        "        return images_batch, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COZCFN75z4x6"
      },
      "source": [
        "### Linear Classification\n",
        "We can use the feature maps in the later convolutional layers of the pretrained model as our learned representation for linear classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhE1_yBqETzw"
      },
      "source": [
        "model, linear_classifier, train_loader, test_loader = load_model_and_data('rotation')\n",
        "evaluate_accuracy(model, linear_classifier, train_loader, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DiA9kai0ncr"
      },
      "source": [
        "### Nearest Neighbors\n",
        "Another way to evaluate our learned representation is to look at nearest neighbors to random encoded images in latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sePwukfM0sef"
      },
      "source": [
        "display_nearest_neighbors('rotation', model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB_cqJagEXbw"
      },
      "source": [
        "## Demo 3: SimCLR [[3]](https://arxiv.org/abs/2002.05709)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuLqNpTq1gkT"
      },
      "source": [
        "SimCLR is a contrastive learning framework to learn strong visual representations. An image $x$ is processed using data augmentation to produce two variants $x_i$ and $x_j$ which are both fed into encoder $f$ (a CNN) and projection head $g$ (a small MLP). The models optimize a contrastive loss to maximally align projected latents $z_i, z_j$. We consider $x_i, x_j$ as a positive pair, and any other $x_i, x_k$ pairs (i.e. different images in the same batch) are negative pairs. A visual diagram of the training procedure is shown below (from the paper).\n",
        "\n",
        "![](https://drive.google.com/uc?id=1XW1uIkUTMSa0DZncivSYXzM5gA5FIhF6)\n",
        "\n",
        "More formally, the loss between positive example $z_i, z_j$ is:\n",
        "$$\\ell_{i,j} = -\\log{\\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i, z_k)/\\tau)}}$$\n",
        "where $\\text{sim}(z_i, z_j) = z_i^Tz_j / (\\left\\Vert z_i \\right\\Vert \\left\\Vert z_j \\right\\Vert)$. The loss function can also be interpreted as a standard cross entropy loss to classify positive samples where logits are constructed using a given similarity function.\n",
        "\n",
        "Note: A common idea in contrastive learning methods is that a larger batch means more negative samples. Therefore, these methods usually benefit the most from large-batch learning compared to other self-supervised learning tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1iYSBMgjrVy"
      },
      "source": [
        "### Example Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExIYNEd3js5v"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from deepul_helper.resnet import resnet_v1\n",
        "from deepul_helper.batch_norm import SyncBatchNorm, BatchNorm1d\n",
        "\n",
        "# Some code adapted from https://github.com/sthalles/SimCLR\n",
        "class SimCLR(nn.Module):\n",
        "    metrics = ['Loss']\n",
        "    metrics_fmt = [':.4e']\n",
        "\n",
        "    def __init__(self, dataset, n_classes, dist=None):\n",
        "        super().__init__()\n",
        "        self.temperature = 0.5\n",
        "        self.projection_dim = 128\n",
        "\n",
        "        if dataset == 'cifar10':\n",
        "            resnet = resnet_v1((3, 32, 32), 50, 1, cifar_stem=True)\n",
        "            resnet = SyncBatchNorm.convert_sync_batchnorm(resnet)\n",
        "            self.resnet = resnet\n",
        "            self.latent_dim = 2048\n",
        "        elif 'imagenet' in dataset:\n",
        "            resnet = resnet_v1((3, 128, 128), 50, 1, cifar_stem=False)\n",
        "            if dist is not None:\n",
        "                resnet = nn.SyncBatchNorm.convert_sync_batchnorm(resnet)\n",
        "            self.resnet = resnet\n",
        "            self.latent_dim = 2048\n",
        "\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(self.latent_dim, self.projection_dim, bias=False),\n",
        "            BatchNorm1d(self.projection_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(self.projection_dim, self.projection_dim, bias=False),\n",
        "            BatchNorm1d(self.projection_dim, center=False)\n",
        "        )\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.n_classes = n_classes\n",
        "        self.dist = dist\n",
        "\n",
        "    def construct_classifier(self):\n",
        "        return nn.Sequential(nn.Linear(self.latent_dim, self.n_classes))\n",
        "\n",
        "    def forward(self, images):\n",
        "        n = images[0].shape[0]\n",
        "        xi, xj = images\n",
        "        hi, hj = self.encode(xi), self.encode(xj) # (N, latent_dim)\n",
        "        zi, zj = self.proj(hi), self.proj(hj) # (N, projection_dim)\n",
        "        zi, zj = F.normalize(zi), F.normalize(zj)\n",
        "\n",
        "        # Each training example has 2N - 2 negative samples\n",
        "        # 2N total samples, but exclude the current and positive sample\n",
        "\n",
        "        if self.dist is None:\n",
        "            zis = [zi]\n",
        "            zjs = [zj]\n",
        "        else:\n",
        "            zis = [torch.zeros_like(zi) for _ in range(self.dist.get_world_size())]\n",
        "            zjs = [torch.zeros_like(zj) for _ in range(self.dist.get_world_size())]\n",
        "\n",
        "            self.dist.all_gather(zis, zi)\n",
        "            self.dist.all_gather(zjs, zj)\n",
        "\n",
        "        z1 = torch.cat((zi, zj), dim=0) # (2N, projection_dim)\n",
        "        z2 = torch.cat(zis + zjs, dim=0) # (2N * n_gpus, projection_dim)\n",
        "\n",
        "        sim_matrix = torch.mm(z1, z2.t()) # (2N, 2N * n_gpus)\n",
        "        sim_matrix = sim_matrix / self.temperature\n",
        "        # Mask out same-sample terms\n",
        "        n_gpus = 1 if self.dist is None else self.dist.get_world_size()\n",
        "        rank = 0 if self.dist is None else self.dist.get_rank()\n",
        "        sim_matrix[torch.arange(n), torch.arange(rank*n, (rank+1)*n)]  = -float('inf')\n",
        "        sim_matrix[torch.arange(n, 2*n), torch.arange((n_gpus+rank)*n, (n_gpus+rank+1)*n)] = -float('inf')\n",
        "\n",
        "        targets = torch.cat((torch.arange((n_gpus+rank)*n, (n_gpus+rank+1)*n),\n",
        "                             torch.arange(rank*n, (rank+1)*n)), dim=0)\n",
        "        targets = targets.to(sim_matrix.get_device()).long()\n",
        "\n",
        "        loss = F.cross_entropy(sim_matrix, targets, reduction='sum')\n",
        "        loss = loss / n\n",
        "        return dict(Loss=loss), hi\n",
        "\n",
        "    def encode(self, images):\n",
        "        return self.resnet(images[0])\n",
        "\n",
        "    def get_features(self, images):\n",
        "        return self.resnet.get_features(images)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6HCMyM24bFQ"
      },
      "source": [
        "### Linear Classification\n",
        "We can use the encoded vector $h_i$ as our latent representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up_uOaJG4W2N"
      },
      "source": [
        "model, linear_classifier, train_loader, test_loader = load_model_and_data('simclr')\n",
        "evaluate_accuracy(model, linear_classifier, train_loader, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mnGhiXc5EuW"
      },
      "source": [
        "### Nearest Neighbors\n",
        "Another way to evaluate our learned representation is to look at nearest neighbors to random encoded images in latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQvrC7fQEZgC"
      },
      "source": [
        "display_nearest_neighbors('simclr', model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqGOS_456L8G"
      },
      "source": [
        "## Other Tasks\n",
        "In addition to the above tasks, prior work has also investigated other self-superivsed tasks such as solving jigsaw puzzles [[4]](https://arxiv.org/abs/1603.09246), cycle-consistency [[5]](https://arxiv.org/abs/1903.07593), contrastive learning [[6]](https://arxiv.org/abs/1807.03748)[[7]](https://arxiv.org/abs/1905.09272)[[8]](https://arxiv.org/abs/1906.05849)[[9]](https://arxiv.org/abs/1911.05722), and patch prediction [[10]](https://arxiv.org/abs/1505.05192). See [here](https://github.com/jason718/awesome-self-supervised-learning) for a great resource on more self-supervised learning papers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UznQoUaiEaCX"
      },
      "source": [
        "# Demo 4: Using Representations for Downstream Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O3ICsfdkGoF"
      },
      "source": [
        "After pretraining a model on a self-supervised learning task, we can use it for other downstream tasks. In this demo, we use the pre-trained ResNet50 backbone from training SimCLR on a subset of ImageNet to learn a semantic segmentation model on Pascal VOC 2012. We use a simple U-Net [[11]](https://arxiv.org/abs/1505.04597) architecture with skip connections across feature maps between the SimCLR encoder and learned upsampling decoder. We do not fine-tune the SimCLR ResNet50 backbone, and only optimize the upsampling portion.\n",
        "\n",
        "![](https://drive.google.com/uc?id=19dxxcwof0IA0jyv0VCl4rnZZf3ajA22s)\n",
        "\n",
        "The training script can be found in `train_segmentation.py` [here](https://github.com/wilson1yan/cs294-158-ssl/blob/master/train_segmentation.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_z7-YFoYCwR"
      },
      "source": [
        "## Example Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZJuWNENYGT9"
      },
      "source": [
        "# Code adapted from https://github.com/qubvel/segmentation_models.pytorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from deepul_helper.resnet import NormReLU\n",
        "\n",
        "class SegmentationModel(nn.Module):\n",
        "    metrics = ['Loss']\n",
        "    metrics_fmt = [':.4e']\n",
        "\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        decoder_channels = (512, 256, 128, 64, 32)\n",
        "        encoder_channels = (2048, 1024, 512, 256, 64) # Starting from head (resnet 50)\n",
        "\n",
        "        # Construct decoder blocks\n",
        "        in_channels = [encoder_channels[0]] + list(decoder_channels[:-1])\n",
        "        skip_channels = list(encoder_channels[1:]) + [0]\n",
        "        out_channels = decoder_channels\n",
        "        blocks = [\n",
        "            DecoderBlock(in_ch, skip_ch, out_ch)\n",
        "            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n",
        "        ]\n",
        "        self.dec_blocks = nn.ModuleList(blocks)\n",
        "\n",
        "        # Segmentation head for output prediction\n",
        "        self.seg_head = nn.Conv2d(decoder_channels[-1], n_classes, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, features, targets):\n",
        "        features = features[1:] # remove first skip with same spatial resolution\n",
        "        features = features[::-1] # reverse channels to start from head of encoder\n",
        "\n",
        "        skips = features[1:]\n",
        "        x = features[0]\n",
        "        for i, decoder_block in enumerate(self.dec_blocks):\n",
        "            skip = skips[i] if i < len(skips) else None\n",
        "            x = decoder_block(x, skip)\n",
        "\n",
        "        logits = self.seg_head(x)\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return dict(Loss=loss), logits\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            skip_channels,\n",
        "            out_channels,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels + skip_channels, out_channels,\n",
        "                      kernel_size=3, padding=1),\n",
        "            NormReLU((out_channels, None, None)), # only care about channel dim for BN\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            NormReLU((out_channels, None, None))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip=None):\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if skip is not None:\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXcDxUwYYHgM"
      },
      "source": [
        "## Segmentation Results\n",
        "Below, we show a random subset of segmentations from the trained model. Every set of 3 images consists of the original image, the labeled segmentation, and the predicted segmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYUmet3REeIa"
      },
      "source": [
        "from deepul_helper.demos import show_segmentation\n",
        "show_segmentation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI94rsx_Efro"
      },
      "source": [
        "# Demo 5: Avoiding Trivial Representations in Self-Supervised Tasks\n",
        "\n",
        "When designing a self-supervised learning task, it is important to make sure that no trivial solutions exists. In general, a learned solution is trivial if the model is able to successfully complete its task by taking advantage of low-level features. As a result, it doesn't learn a good representation so downstream performance is bad.\n",
        "\n",
        "For example, in the jigsaw [[4]](https://arxiv.org/abs/1603.09246) task, a model can \"cheat\" by just looking at the boundary textures of the jigsaw pieces, or following and matching straight lines across different pieces. These issues can generally be fixed by ranndom cropping, shifting, and spacially jittering.\n",
        "\n",
        "We look at two other less obvious aspects of images that may reduce performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGmE4AFUGgi3"
      },
      "source": [
        "## Chromatic Aberration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLKZH3vOTRtM"
      },
      "source": [
        "Chromatic aberration occurs when the different focal lengths of light results in the light not meeting all at the same point.\n",
        "![from wikipedia](https://drive.google.com/uc?id=1PYGoQWnH0aAeiE_8t4ef5WDcq1UIQQ5t)\n",
        "\n",
        "A example of very apparent chromatic aberration is shown below, where the green and magenta colors are clearly offset with each other:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1M1B6kV6ddBwyJse3FQT8_XBTeqs5s5WL)\n",
        "\n",
        "Chromatic aberration generally becomes a problem in patch-based self-supervised learning tasks that design, such as solving jigsaw puzzles, or predicintg the correct location of a patch in an image In this case, the model can take advantage of the low-level chromatic aberration features to get a strong idea of where the patch is located without understanding the actual context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lcNZIa9VHw-"
      },
      "source": [
        "Below is a quick demo of chromatic aberration in more realistic images, and possible fixes. Note that in general, chromatic aberration is fairly hard to spot with the naked eye, but deep learning models are still able to use it to their advantage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4A0BVarEp4C"
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# You can see some chromatic aberration in the purple fringes around the branches\n",
        "\n",
        "image = Image.open('sample_images/chrom_ab_demo.png')\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViS8riD8WEyr"
      },
      "source": [
        "Chromatic aberration is generally fixed through conversion to grayscale, or color dropping. Color dropping works by dropping 2 of the color channels and replacing them with random noise uniform or Gaussian noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXyV8JP2WES8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Color Dropping\n",
        "# We will drop all channels except R\n",
        "image_cpy = image.copy()\n",
        "pixels = image_cpy.load()\n",
        "\n",
        "arr = np.array(image_cpy)\n",
        "std_R = np.std(arr[:, :, 0])\n",
        "mean_G, mean_B = np.mean(arr[:, :, 1]), np.mean(arr[:, :, 2])\n",
        "\n",
        "for i in range(image.size[0]):\n",
        "  for j in range(image.size[1]):\n",
        "    p = pixels[i, j] # (R, G, B, A)\n",
        "    R, A = p[0], p[3]\n",
        "    G = int(np.random.randn() * std_R + mean_G)\n",
        "    B = int(np.random.randn() * std_R + mean_B)\n",
        "    pixels[i, j] = (R, G, p[2], p[3])\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Color Dropping')\n",
        "plt.axis('off')\n",
        "plt.imshow(image_cpy)\n",
        "plt.show()\n",
        "\n",
        "# Grayscale\n",
        "image_cpy2 = image.copy()\n",
        "pixels2 = image_cpy2.load()\n",
        "\n",
        "for i in range(image.size[0]):\n",
        "  for j in range(image.size[1]):\n",
        "    p = pixels[i, j]\n",
        "    G = int(0.3 * p[0] + 0.59 * p[1] + 0.11 * p[2])\n",
        "    pixels2[i, j] = (G, G, G, 255)\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Grayscale')\n",
        "plt.axis('off')\n",
        "plt.imshow(image_cpy2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHDJB6XdH9bq"
      },
      "source": [
        "## Color Intensity Histograms\n",
        "\n",
        "In the SimCLR paper, the authors show that the histogram of color intensities of different patches within the same image have very similar histograms, which may degrade training by encouraging models to look at low-level (pixel intensity) features to solve self-supervised tasks that involve matching positive patches of the same image.\n",
        "\n",
        "Below, we run a similar demo to what was demonstrated in the paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPiDcnVgICJV"
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image = Image.open('sample_images/n01537544_19414.JPEG')\n",
        "plt.figure()\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q47ATyUihBv6"
      },
      "source": [
        "import numpy as np\n",
        "arr = np.array(image)\n",
        "H, W, _ = arr.shape\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "for i in range(4):\n",
        "  r = np.random.randint(0, H - 128)\n",
        "  c = np.random.randint(0, W - 128)\n",
        "  patch = arr[r:r+128, c:c+128]\n",
        "\n",
        "  axs[i].set_title(f'Patch {i}')\n",
        "  axs[i].hist(patch.reshape(-1), bins=50)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5F_c4JDi6PX"
      },
      "source": [
        "Now we apply color jittering to mitigate this effect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQicXcDwhNif"
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
        "jitter_img = color_jitter(image)\n",
        "\n",
        "image = Image.open('sample_images/n01537544_19414.JPEG')\n",
        "plt.figure()\n",
        "plt.title('Color Jittered Image')\n",
        "plt.axis('off')\n",
        "plt.imshow(jitter_img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOPbrPuXiqjc"
      },
      "source": [
        "arr = np.array(image)\n",
        "H, W, _ = arr.shape\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "for i in range(4):\n",
        "  r = np.random.randint(0, H - 128)\n",
        "  c = np.random.randint(0, W - 128)\n",
        "  patch = arr[r:r+128, c:c+128]\n",
        "\n",
        "  axs[i].set_title(f'Patch {i}')\n",
        "  axs[i].hist(patch.reshape(-1), bins=50)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnlJmV51i1vJ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frantz Dervis\n"
      ],
      "metadata": {
        "id": "_bqdAXibBPiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3\n"
      ],
      "metadata": {
        "id": "j_7ZAzXOBWGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from deepul_helper.demos import evaluate_classifier\n",
        "\n",
        "# Evaluate Context Encoder\n",
        "model, linear_classifier, train_loader, test_loader = load_model_and_data('context_encoder')\n",
        "train_acc1_ce, train_acc5_ce = evaluate_classifier(model, linear_classifier, train_loader)\n",
        "test_acc1_ce, test_acc5_ce = evaluate_classifier(model, linear_classifier, test_loader)\n",
        "\n",
        "# Evaluate Rotation Prediction\n",
        "model, linear_classifier, train_loader, test_loader = load_model_and_data('rotation')\n",
        "train_acc1_rot, train_acc5_rot = evaluate_classifier(model, linear_classifier, train_loader)\n",
        "test_acc1_rot, test_acc5_rot = evaluate_classifier(model, linear_classifier, test_loader)\n",
        "\n",
        "# Evaluate SimCLR\n",
        "model, linear_classifier, train_loader, test_loader = load_model_and_data('simclr')\n",
        "train_acc1_simclr, train_acc5_simclr = evaluate_classifier(model, linear_classifier, train_loader)\n",
        "test_acc1_simclr, test_acc5_simclr = evaluate_classifier(model, linear_classifier, test_loader)\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results = pd.DataFrame({\n",
        "    'Model': ['Context Encoder', 'Rotation Prediction', 'SimCLR'],\n",
        "    'Train Top 1 Accuracy': [train_acc1_ce, train_acc1_rot, train_acc1_simclr],\n",
        "    'Train Top 5 Accuracy': [train_acc5_ce, train_acc5_rot, train_acc5_simclr],\n",
        "    'Test Top 1 Accuracy': [test_acc1_ce, test_acc1_rot, test_acc1_simclr],\n",
        "    'Test Top 5 Accuracy': [test_acc5_ce, test_acc5_rot, test_acc5_simclr]\n",
        "})\n",
        "\n",
        "display(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "mCSV_WxbBi4B",
        "outputId": "21a7f305-7f08-470e-9ada-5d9b20bcbf5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                 Model  Train Top 1 Accuracy  Train Top 5 Accuracy  \\\n",
              "0      Context Encoder                53.236                94.094   \n",
              "1  Rotation Prediction                79.610                99.166   \n",
              "2               SimCLR                90.272                99.474   \n",
              "\n",
              "   Test Top 1 Accuracy  Test Top 5 Accuracy  \n",
              "0                45.77                90.29  \n",
              "1                79.91                99.12  \n",
              "2                92.84                99.86  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ba342a2-840f-4c0e-8066-b03c6a0f62c6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Train Top 1 Accuracy</th>\n",
              "      <th>Train Top 5 Accuracy</th>\n",
              "      <th>Test Top 1 Accuracy</th>\n",
              "      <th>Test Top 5 Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Context Encoder</td>\n",
              "      <td>53.236</td>\n",
              "      <td>94.094</td>\n",
              "      <td>45.77</td>\n",
              "      <td>90.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rotation Prediction</td>\n",
              "      <td>79.610</td>\n",
              "      <td>99.166</td>\n",
              "      <td>79.91</td>\n",
              "      <td>99.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SimCLR</td>\n",
              "      <td>90.272</td>\n",
              "      <td>99.474</td>\n",
              "      <td>92.84</td>\n",
              "      <td>99.86</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ba342a2-840f-4c0e-8066-b03c6a0f62c6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7ba342a2-840f-4c0e-8066-b03c6a0f62c6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7ba342a2-840f-4c0e-8066-b03c6a0f62c6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-25d8bad3-23a0-4444-92c3-9004e59c5ba6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-25d8bad3-23a0-4444-92c3-9004e59c5ba6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-25d8bad3-23a0-4444-92c3-9004e59c5ba6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_dadce76c-2beb-439c-ac14-3e48ba4758cc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_dadce76c-2beb-439c-ac14-3e48ba4758cc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results",
              "summary": "{\n  \"name\": \"results\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Context Encoder\",\n          \"Rotation Prediction\",\n          \"SimCLR\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Train Top 1 Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19.065376191760116,\n        \"min\": 53.236,\n        \"max\": 90.272,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          53.236,\n          79.61,\n          90.272\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Train Top 5 Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.021160042103037,\n        \"min\": 94.094,\n        \"max\": 99.474,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          94.094,\n          99.166,\n          99.474\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Test Top 1 Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24.318406608986535,\n        \"min\": 45.77,\n        \"max\": 92.84,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          45.77,\n          79.91,\n          92.84\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Test Top 5 Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.324493716151171,\n        \"min\": 90.29,\n        \"max\": 99.86,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          90.29,\n          99.12,\n          99.86\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "2fc7d28b",
        "outputId": "796d2f48-35d8-4ace-946f-a8c8d0a649d1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "models = results['Model']\n",
        "test_acc1 = results['Test Top 1 Accuracy']\n",
        "test_acc5 = results['Test Top 5 Accuracy']\n",
        "\n",
        "ax[0].bar(models, test_acc1, color=['blue', 'green', 'red'])\n",
        "ax[0].set_ylabel('Accuracy (%)')\n",
        "ax[0].set_title('Test Top 1 Accuracy')\n",
        "ax[0].set_yscale('log')\n",
        "ax[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "ax[1].bar(models, test_acc5, color=['blue', 'green', 'red'])\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_title('Test Top 5 Accuracy')\n",
        "ax[1].set_yscale('log') # Set y-axis to log scale\n",
        "ax[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#log scale allows to see the difference between rotation pred and SimCLR more acurately"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAHqCAYAAADVi/1VAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAg0tJREFUeJzs3XlYlPX+//HXDLuKpqEoLrim4oK7WbkhaVSmpOaapqlllAsnPZknl8wlSzOVNDu54Ubm0qK5YYka5m56LFPDFUXJHVSWuX9/9GO+EZCAMMPyfFwX13Hu+dz3vGeYw7x6z+f+3CbDMAwBAAAAAAAANmS2dwEAAAAAAAAofGhKAQAAAAAAwOZoSgEAAAAAAMDmaEoBAAAAAADA5mhKAQAAAAAAwOZoSgEAAAAAAMDmaEoBAAAAAADA5mhKAQAAAAAAwOZoSgEAAAAAAMDmaEoBAAAAAADA5mhKAYWMyWTK1M8PP/zwwI8VHx+v8ePHZ+pYlStXzlRdixYteuC67icsLEx9+vRRjRo1ZDKZ1KZNm2wd55dffpHJZJKrq6uuX7+eozUCAIC8gWx1fxnV8uqrr2bpOBs2bJDJZJKXl5csFksuVQvAlhztXQAA2woNDU11e8mSJdqyZUua7bVr137gx4qPj9eECRMk6b6NnZkzZ+r27dvW2xs2bNCKFSv00UcfycPDw7r9sccee+C67mfu3Lnav3+/mjZtqj/++CPbx1m6dKnKli2ra9eu6csvv9TAgQNzsEoAAJAXkK0yp0GDBvrXv/6VatsjjzySpWMsW7ZMlStX1unTp7Vt2zb5+/vnZIkA7ICmFFDI9OnTJ9Xt3bt3a8uWLWm221rnzp1T3b506ZJWrFihzp07q3LlyjatJTQ0VOXLl5fZbFbdunWzdQzDMLR8+XL16tVLUVFRWrZsWZ5tSsXFxalo0aL2LgMAgHyJbJU55cuXf6DXJC4uTl999ZWmTJmihQsXatmyZXm2KUW2AjKP0/cApGGxWDRz5kzVqVNHrq6u8vT01CuvvKJr166lGrdv3z516NBBHh4ecnNzU5UqVTRgwABJ0unTp1W6dGlJ0oQJE6zTtMePH5/tupKSkjRx4kRVq1ZNLi4uqly5st5++23du3cv1bjKlSvr2Wef1ebNm9WgQQO5urrKx8dHa9asydTjVKxYUWbzg/153LVrl06fPq0ePXqoR48eioiI0Pnz59OMs1gs+vjjj1WvXj25urqqdOnSeuqpp7Rv375U45YuXapmzZqpSJEiKlmypFq1aqXNmzdb78/ota1cubJeeukl6+1FixbJZDJp+/bteu2111SmTBlVqFBBknTmzBm99tprqlmzptzc3PTwww+rW7duOn36dJrjXr9+XSNGjFDlypXl4uKiChUqqG/fvoqNjdXt27dVtGhRDRs2LM1+58+fl4ODg6ZMmZLJVxIAgPyvsGerFAkJCYqLi8tWrWvXrtWdO3fUrVs39ejRQ2vWrNHdu3fTjLt7967Gjx+vRx55RK6uripXrpyef/55nTp1yjrmfvnr9OnTGZ7a+PfXfPz48TKZTDp27Jh69eqlkiVL6oknnpAk/fzzz3rppZdUtWpVubq6qmzZshowYEC6M/EvXLigl19+WV5eXnJxcVGVKlU0ZMgQJSQk6Pfff5fJZNJHH32UZr8ff/xRJpNJK1asyOpLCuQJNKUApPHKK69o5MiRevzxx/Xxxx+rf//+WrZsmTp06KDExERJ0uXLl9W+fXudPn1ab731lmbPnq3evXtr9+7dkqTSpUtr7ty5kqTAwECFhoYqNDRUzz//fLbrGjhwoMaOHatGjRrpo48+UuvWrTVlyhT16NEjzdgTJ06oe/fuCggI0JQpU+To6Khu3bppy5Yt2X78rFi2bJmqVaumpk2bqmPHjipSpEi6YeHll1/W8OHDVbFiRb3//vt666235Orqan0dpT+D54svvignJye9++67mjBhgipWrKht27Zlu77XXntNx44d09ixY/XWW29Jkvbu3asff/xRPXr00KxZs/Tqq68qPDxcbdq0UXx8vHXf27dvq2XLlpo9e7bat2+vjz/+WK+++qp+/fVXnT9/XsWKFVNgYKDCwsKUnJyc6nFXrFghwzDUu3fvbNcOAEB+Q7aStm3bpiJFiqhYsWKqXLmyPv744yzVumzZMrVt21Zly5ZVjx49dOvWLX3zzTepxiQnJ+vZZ5/VhAkT1LhxY02fPl3Dhg3TjRs3dPToUeu4zOSvrOrWrZvi4+M1efJkDRo0SJK0ZcsW/f777+rfv79mz56tHj16aOXKlXr66adlGIZ13+joaDVr1kwrV65U9+7dNWvWLL344ovavn274uPjVbVqVT3++ONatmxZuq+Lu7u7OnXqlO3aAbsyABRqQUFBxl//FOzYscOQZCxbtizVuI0bN6bavnbtWkOSsXfv3gyPfeXKFUOSMW7cuCzX9cEHHxiSjKioKMMwDOPQoUOGJGPgwIGpxr355puGJGPbtm3Wbd7e3oYkY/Xq1dZtN27cMMqVK2c0bNgwS3XUqVPHaN26dZb2SUhIMB5++GFjzJgx1m29evUyfH19U43btm2bIckYOnRommNYLBbDMAzjxIkThtlsNgIDA43k5OR0xxiGkeHr7O3tbfTr1896e+HChYYk44knnjCSkpJSjY2Pj0+zf2RkpCHJWLJkiXXb2LFjDUnGmjVrMqx706ZNhiTju+++S3V//fr1s/x6AgCQn5Ct0urYsaPx/vvvG+vWrTM+//xzo2XLloYkY9SoUZmqPSYmxnB0dDQ+++wz67bHHnvM6NSpU6pxCxYsMCQZM2bMSHOMlIySmfwVFRVlSDIWLlyYZszfX/9x48YZkoyePXumGZtetlqxYoUhyYiIiLBu69u3r2E2m9P93afU9OmnnxqSjF9++cV6X0JCguHh4ZEq6wH5DTOlAKSyatUqlShRQk8++aRiY2OtP40bN1axYsX0/fffS5IeeughSdK3335r/YYvN23YsEGSFBwcnGp7yoKZ69evT7Xdy8tLgYGB1tvFixdX3759dfDgQV26dClXa/3uu+/0xx9/qGfPntZtPXv21OHDh/W///3Pum316tUymUwaN25cmmOYTCZJ0rp162SxWDR27Ng0pxSmjMmOQYMGycHBIdU2Nzc3678TExP1xx9/qHr16nrooYd04MCBVHX7+vqmen3/XpO/v7+8vLxSfaN39OhR/fzzz3ZfYwMAAFsiW0lff/21Ro0apU6dOmnAgAHavn27OnTooBkzZqS7vMHfrVy5UmazWV26dLFu69mzp7777rtUp0CuXr1aHh4eeuONN9IcIyWjZCZ/ZUd6VxL8a7a6e/euYmNj9eijj0qSNVtZLBatW7dOHTt2VJMmTTKs6YUXXpCrq2uqbLVp0ybFxsaSrZCv0ZQCkMqJEyd048YNlSlTRqVLl071c/v2bV2+fFmS1Lp1a3Xp0kUTJkyQh4eHOnXqpIULF6ZZgyCnnDlzRmazWdWrV0+1vWzZsnrooYd05syZVNurV6+eJlikXOElvTWSctLSpUtVpUoVubi46OTJkzp58qSqVaumIkWKpAoSp06dkpeXl0qVKpXhsU6dOiWz2SwfH58crbFKlSpptt25c0djx45VxYoV5eLiIg8PD5UuXVrXr1/XjRs3UtV0vwXgzWazevfurXXr1llP/Vu2bJlcXV3VrVu3HH0uAADkZWSrtEwmk0aMGKGkpCT98MMP9x2fsrbmH3/8Yc1WDRs2VEJCglatWmUdd+rUKdWsWVOOjhlfzysz+Ss70stWV69e1bBhw+Tp6Sk3NzeVLl3aOi4lW125ckU3b968b7Z66KGH1LFjRy1fvty6bdmyZSpfvrz8/Pxy8JkAtsXV9wCkYrFYVKZMmXTPWZdkXWDTZDLpyy+/1O7du/XNN99o06ZNGjBggKZPn67du3erWLFiuVLfg3yDZQs3b97UN998o7t376pGjRpp7l++fLkmTZpks+fx9zWdUvz1m7sUb7zxhhYuXKjhw4erRYsWKlGihEwmk3r06CGLxZLlx+7bt68++OADrVu3Tj179tTy5cv17LPPqkSJElk+FgAA+RXZKn0VK1aU9Gfj5p+cOHFCe/fulaR0s9WyZcs0ePDgHK0to9cko1wlpZ+tXnjhBf34448aOXKkGjRooGLFisliseipp57KdrZatWqVfvzxR9WrV09ff/21XnvttQe+QA9gTzSlAKRSrVo1bd26VY8//ni6H65/9+ijj+rRRx/VpEmTtHz5cvXu3VsrV67UwIEDczTkeHt7y2Kx6MSJE6pdu7Z1e0xMjK5fvy5vb+9U40+ePCnDMFLV8Ntvv0lSrl4GOeVKMHPnzpWHh0eq+44fP67//Oc/2rVrl5544glVq1ZNmzZt0tWrVzP8tq5atWqyWCw6duyYGjRokOHjlixZUtevX0+1LSEhQRcvXsx07V9++aX69eun6dOnW7fdvXs3zXGrVauWarHQjNStW1cNGzbUsmXLVKFCBZ09e1azZ8/OdD0AABQEZKv0/f7775L+rymXkWXLlsnJyUmhoaFplh7YuXOnZs2apbNnz6pSpUqqVq2afvrpJyUmJsrJySnd42Umf5UsWVKS0mSgv88e+yfXrl1TeHi4JkyYoLFjx1q3nzhxItW40qVLq3jx4pnKVk899ZRKly6tZcuWqXnz5oqPj9eLL76Y6ZqAvIiWKoBUXnjhBSUnJ2vixIlp7ktKSrJ+OF+7di3VVUMkWZsmKdPMixQpIintB3p2PP3005KkmTNnpto+Y8YMSdIzzzyTant0dLTWrl1rvX3z5k0tWbJEDRo0UNmyZR+4nowsXbpUVatW1auvvqquXbum+nnzzTdVrFgx6zelXbp0kWEYmjBhQprjpLy2nTt3ltls1rvvvpvmG7W/vv7VqlVTREREqvvnz5//j9/o/Z2Dg0Oa3+ns2bPTHKNLly46fPhwqtc3vZok6cUXX9TmzZs1c+ZMPfzwwwoICMh0PQAAFASFPVtdvXo1TZZITEzU1KlT5ezsrLZt2/5jncuWLVPLli3VvXv3NNlq5MiRkmS9wnGXLl0UGxurOXPmpDlOymubmfxVvHhxeXh4pMlWn3zyyT/W+lcpDbS//07//nqbzWZ17txZ33zzjfbt25dhTZLk6Oionj176osvvtCiRYtUr1491a9fP9M1AXkRM6UApNK6dWu98sormjJlig4dOqT27dvLyclJJ06c0KpVq/Txxx+ra9euWrx4sT755BMFBgaqWrVqunXrlj777DMVL17cGnLc3Nzk4+OjsLAwPfLIIypVqpTq1q1733Pm0+Pr66t+/fpp/vz5un79ulq3bq09e/Zo8eLF6ty5c5pA88gjj+jll1/W3r175enpqQULFigmJkYLFy6872NFRERYQ8iVK1cUFxen9957T5LUqlUrtWrVKt39oqOj9f3332vo0KHp3u/i4qIOHTpo1apVmjVrltq2basXX3xRs2bN0okTJ6xTuXfs2KG2bdvq9ddfV/Xq1TVmzBhNnDhRLVu21PPPPy8XFxft3btXXl5emjJliqQ/L+n86quvqkuXLnryySd1+PBhbdq0Kc1srX/y7LPPKjQ0VCVKlJCPj48iIyO1detWPfzww6nGjRw5Ul9++aW6deumAQMGqHHjxrp69aq+/vprzZs3T76+vtaxvXr10qhRo7R27VoNGTIkw28tAQAoqAp7tvr666/13nvvqWvXrqpSpYquXr2q5cuX6+jRo5o8efI/NrR++uknnTx5Uq+//nq695cvX16NGjXSsmXL9O9//1t9+/bVkiVLFBwcrD179qhly5aKi4vT1q1b9dprr6lTp06Zyl/Sn9lq6tSpGjhwoJo0aaKIiAjrzLDMKF68uFq1aqVp06YpMTFR5cuX1+bNmxUVFZVm7OTJk7V582a1bt1agwcPVu3atXXx4kWtWrVKO3futC6CL/15Ct+sWbP0/fff6/333890PUCeZY9L/gHIO/5+2eIU8+fPNxo3bmy4ubkZ7u7uRr169YxRo0YZ0dHRhmEYxoEDB4yePXsalSpVMlxcXIwyZcoYzz77rLFv375Ux/nxxx+Nxo0bG87Ozlm6hPHfL1tsGIaRmJhoTJgwwahSpYrh5ORkVKxY0Rg9erRx9+7dVPt6e3sbzzzzjLFp0yajfv36houLi1GrVi1j1apVmXrslEv7pvfzT/VPnz7dkGSEh4dnOGbRokWGJOOrr74yDMMwkpKSjA8++MCoVauW4ezsbJQuXdoICAgw9u/fn2q/BQsWGA0bNjRcXFyMkiVLGq1btza2bNlivT85Odn497//bXh4eBhFihQxOnToYJw8edLw9vZOdZnghQsXZni56WvXrhn9+/c3PDw8jGLFihkdOnQwfv311zTHMAzD+OOPP4zXX3/dKF++vOHs7GxUqFDB6NevnxEbG5vmuE8//bQhyfjxxx8zfF0AACgoyFap7du3z+jYsaM1MxQrVsx44oknjC+++OK++77xxhuGJOPUqVMZjhk/frwhyTh8+LBhGIYRHx9vjBkzxvqcypYta3Tt2jXVMTKTv+Lj442XX37ZKFGihOHu7m688MILxuXLl9O85im58cqVK2lqO3/+vBEYGGg89NBDRokSJYxu3boZ0dHR6f7ezpw5Y/Tt29coXbq04eLiYlStWtUICgoy7t27l+a4derUMcxms3H+/Pn7voZAXmcyjL/NJwSAfK5y5cqqW7euvv32W3uXAkmBgYE6cuSITp48ae9SAABANpCt8paGDRuqVKlSCg8Pt3cpwANjTSkAQK65ePGi1q9fzyKcAAAAOWDfvn06dOiQ+vbta+9SgBzBmlIAgBwXFRWlXbt26b///a+cnJz0yiuv2LskAACAfOvo0aPav3+/pk+frnLlyql79+72LgnIEcyUAgDkuO3bt+vFF19UVFSUFi9enKtXPAQAACjovvzyS/Xv31+JiYlasWKFXF1d7V0SkCNYUwoAAAAAAAA2x0wpAAAAAAAA2BxNKQAAAAAAANgcC53nIovFoujoaLm7u8tkMtm7HAAAkAHDMHTr1i15eXnJbOY7u7yKbAUAQP6Q2WxFUyoXRUdHq2LFivYuAwAAZNK5c+dUoUIFe5eBDJCtAADIX+6XrWhK5SJ3d3dJf/4SihcvbudqAABARm7evKmKFStaP7uRN5GtAADIHzKbrWhK5aKUaeXFixcnOAEAkA9wSljeRrYCACB/uV+2YtEEAAAAAAAA2BxNKQAAAAAAANgcTSkAAADYxLlz59SmTRv5+Piofv36WrVqlb1LAgAAdsSaUgAAALAJR0dHzZw5Uw0aNNClS5fUuHFjPf300ypatKi9SwMAAHZAUwoAAAA2Ua5cOZUrV06SVLZsWXl4eOjq1as0pQAAKKQ4fQ8AAACZEhERoY4dO8rLy0smk0nr1q1LMyYkJESVK1eWq6urmjdvrj179qR7rP379ys5OVkVK1bM5aoBAEBeRVMKAAAAmRIXFydfX1+FhISke39YWJiCg4M1btw4HThwQL6+vurQoYMuX76catzVq1fVt29fzZ8/3xZlAwCAPIrT9wAAAJApAQEBCggIyPD+GTNmaNCgQerfv78kad68eVq/fr0WLFigt956S5J07949de7cWW+99ZYee+yxf3y8e/fu6d69e9bbN2/ezIFnAQAA8gpmSgEAAOCBJSQkaP/+/fL397duM5vN8vf3V2RkpCTJMAy99NJL8vPz04svvnjfY06ZMkUlSpSw/nCqHwAABQtNKQAAADyw2NhYJScny9PTM9V2T09PXbp0SZK0a9cuhYWFad26dWrQoIEaNGigI0eOZHjM0aNH68aNG9afc+fO5epzAAAAtsXpewAAALCJJ554QhaLJdPjXVxc5OLikosVAQAAe2KmFAAAAB6Yh4eHHBwcFBMTk2p7TEyMypYta6eqAABAXkZTCgAAAA/M2dlZjRs3Vnh4uHWbxWJReHi4WrRoYcfKAABAXsXpewCA/MVksncFsCfDsHcFhdrt27d18uRJ6+2oqCgdOnRIpUqVUqVKlRQcHKx+/fqpSZMmatasmWbOnKm4uDjr1fgAAHkU+apws2O+oikFAACATNm3b5/atm1rvR0cHCxJ6tevnxYtWqTu3bvrypUrGjt2rC5duqQGDRpo48aNaRY/BwAAkCSTYfCVY265efOmSpQooRs3bqh48eL2LgcACga+ySvccim28JmdP/B7AoBcQr4q3HIhX2X2M5s1pQAAAAAAAGBzNKUAAAAAAABgczSlAAAAAAAAYHM0pQAAAAAAAGBzNKWyIDAwUCVLllTXrl3tXQoAAAAAAEC+RlMqC4YNG6YlS5bYuwwAAAAAAIB8j6ZUFrRp00bu7u72LgMAAAAAACDfs3tT6tatWxo+fLi8vb3l5uamxx57THv37s3Rx4iIiFDHjh3l5eUlk8mkdevWpTsuJCRElStXlqurq5o3b649e/bkaB0AAAAAAAD4k92bUgMHDtSWLVsUGhqqI0eOqH379vL399eFCxfSHb9r1y4lJiam2X7s2DHFxMSku09cXJx8fX0VEhKSYR1hYWEKDg7WuHHjdODAAfn6+qpDhw66fPly9p4YAAAAAAAAMmTXptSdO3e0evVqTZs2Ta1atVL16tU1fvx4Va9eXXPnzk0z3mKxKCgoSL169VJycrJ1+/Hjx+Xn56fFixen+zgBAQF67733FBgYmGEtM2bM0KBBg9S/f3/5+Pho3rx5KlKkiBYsWPDgTxQAAAAAAACp2LUplZSUpOTkZLm6uqba7ubmpp07d6YZbzabtWHDBh08eFB9+/aVxWLRqVOn5Ofnp86dO2vUqFHZqiMhIUH79++Xv79/qsfy9/dXZGRklo8XEhIiHx8fNW3aNFv1AAAAAAAAFHR2bUq5u7urRYsWmjhxoqKjo5WcnKylS5cqMjJSFy9eTHcfLy8vbdu2TTt37lSvXr3k5+cnf3//dGdWZVZsbKySk5Pl6emZarunp6cuXbpkve3v769u3bppw4YNqlChQoYNq6CgIB07dizH18YCAAAAAAAoKBztXUBoaKgGDBig8uXLy8HBQY0aNVLPnj21f//+DPepVKmSQkND1bp1a1WtWlWff/65TCZTrte6devWXH8MAAAAAACAwsDuC51Xq1ZN27dv1+3bt3Xu3Dnt2bNHiYmJqlq1aob7xMTEaPDgwerYsaPi4+M1YsSIB6rBw8NDDg4OaRZKj4mJUdmyZR/o2AAAAAAAAEjL7k2pFEWLFlW5cuV07do1bdq0SZ06dUp3XGxsrNq1a6fatWtrzZo1Cg8PV1hYmN58881sP7azs7MaN26s8PBw6zaLxaLw8HC1aNEi28cFAAAAAABA+ux++t6mTZtkGIZq1qypkydPauTIkapVq5b69++fZqzFYlFAQIC8vb0VFhYmR0dH+fj4aMuWLfLz81P58uXTnTV1+/ZtnTx50no7KipKhw4dUqlSpVSpUiVJUnBwsPr166cmTZqoWbNmmjlzpuLi4tKtAwAAAAAAAA/G7k2pGzduaPTo0Tp//rxKlSqlLl26aNKkSXJyckoz1mw2a/LkyWrZsqWcnZ2t2319fbV161aVLl063cfYt2+f2rZta70dHBwsSerXr58WLVokSerevbuuXLmisWPH6tKlS2rQoIE2btyYZvFzAAAAAAAAPDiTYRiGvYsoqG7evKkSJUroxo0bKl68uL3LAYCCwQYXtkAelkuxhc/s/IHfEwDkEvJV4ZYL+Sqzn9l5Zk0pAAAAAAAAFB52P30PAAAAAAo70wRmqhRmxjhOYELhxEwpAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAA2EVgYKBKliyprl272rsUAABgBzSlAAAAYBfDhg3TkiVL7F0GAACwE5pSAAAAsIs2bdrI3d3d3mUAAAA7oSkFAACQR926dUvDhw+Xt7e33Nzc9Nhjj2nv3r0Zjk9OTtY777yjKlWqyM3NTdWqVdPEiRNlGEaO1hUREaGOHTvKy8tLJpNJ69atS3dcSEiIKleuLFdXVzVv3lx79uzJ0ToAAED+RlMKAAAgjxo4cKC2bNmi0NBQHTlyRO3bt5e/v78uXLiQ7vj3339fc+fO1Zw5c/TLL7/o/fff17Rp0zR79uwMH2PXrl1KTExMs/3YsWOKiYlJd5+4uDj5+voqJCQkw+OGhYUpODhY48aN04EDB+Tr66sOHTro8uXL93nWAACgsKApBQAAkAfduXNHq1ev1rRp09SqVStVr15d48ePV/Xq1TV37tx09/nxxx/VqVMnPfPMM6pcubK6du2q9u3bZzhDyWKxKCgoSL169VJycrJ1+/Hjx+Xn56fFixenu19AQIDee+89BQYGZlj/jBkzNGjQIPXv318+Pj6aN2+eihQpogULFmThVQAAAAUZTSkAAIA8KCkpScnJyXJ1dU213c3NTTt37kx3n8cee0zh4eH67bffJEmHDx/Wzp07FRAQkO54s9msDRs26ODBg+rbt68sFotOnTolPz8/de7cWaNGjcpW7QkJCdq/f7/8/f1TPZa/v78iIyOzfLyQkBD5+PioadOm2aoHAADkTTSlsoDLFgMAAFtxd3dXixYtNHHiREVHRys5OVlLly5VZGSkLl68mO4+b731lnr06KFatWrJyclJDRs21PDhw9W7d+8MH8fLy0vbtm3Tzp071atXL/n5+cnf3z/D2ViZERsbq+TkZHl6eqba7unpqUuXLllv+/v7q1u3btqwYYMqVKiQYcMqKChIx44d+8f1tAAAQP5DUyoLuGwxAACwpdDQUBmGofLly8vFxUWzZs1Sz549ZTanH+G++OILLVu2TMuXL9eBAwe0ePFiffjhhxmehpeiUqVKCg0NVVhYmBwdHfX555/LZDLlxlNKZevWrbpy5Yri4+N1/vx5tWjRItcfEwAA5B00pbKAyxYDAABbqlatmrZv367bt2/r3Llz2rNnjxITE1W1atV0x48cOdI6W6pevXp68cUXNWLECE2ZMuUfHycmJkaDBw9Wx44dFR8frxEjRjxQ3R4eHnJwcEizUHpMTIzKli37QMcGAAAFh92bUra4dDGXLQYAAPlZ0aJFVa5cOV27dk2bNm1Sp06d0h0XHx+fZhaVg4ODLBZLhseOjY1Vu3btVLt2ba1Zs0bh4eEKCwvTm2++me16nZ2d1bhxY4WHh1u3WSwWhYeHMxsKAABY2b0pldVLF3PZYgAAUFhs2rRJGzduVFRUlLZs2aK2bduqVq1a6t+/vyRpzpw5ateunXV8x44dNWnSJK1fv16nT5/W2rVrNWPGjAyvkmexWBQQECBvb2/rqXs+Pj7asmWLFi5cqI8++ijd/W7fvq1Dhw7p0KFDkqSoqCgdOnRIZ8+etY4JDg7WZ599psWLF+uXX37RkCFDFBcXZ60dAADA0d4F/PXSxZJUuXJlrVixIt1ZSimXLa5Ro4ZWrlwpBwcHSf932eLg4OB0rxITEBCQ4VVnUvz1ssWSNG/ePK1fv14LFizQW2+99aBPEwAAIMtu3Lih0aNH6/z58ypVqpS6dOmiSZMmycnJSdKfs5xOnTplHT979my98847eu2113T58mV5eXnplVde0dixY9M9vtls1uTJk9WyZUs5Oztbt/v6+mrr1q0qXbp0uvvt27dPbdu2td4ODg6WJPXr10+LFi2SJHXv3l1XrlzR2LFjdenSJTVo0EAbN25Ms/g5AAAovExGTp4nlw2TJ0/W/PnztXnzZj3yyCM6fPiw2rdvrxkzZqR7pZjo6Gi1atVKzZs3V2hoqKKiotSqVSt17NhR8+bNu+/jmUwmrV27Vp07d7ZuS0hIUJEiRfTll1+m2t6vXz9dv35dX331lXXbDz/8oDlz5ujLL7+872PdvHlTJUqU0I0bN1S8ePH7jgcAZIINFl9GHpZLsYXP7PyB3xMKMtMEPt8KM2OcXf+znHxV2OVCvsrsZ7bdZ0q99dZbunnzpmrVqiUHBwclJydr0qRJGV66OOWyxS1btlSvXr0UGRmZq5ct/vXXX623/f39dfjwYcXFxalChQpatWpVuusihISEKCQkRMnJydmuCcjLCE2Fm91DEwAAAIACwe5Nqb9eurhOnTo6dOiQhg8fLi8vL/Xr1y/dfVIuW9y6dWtVrVrVppctzoygoCAFBQVZO4MAAAAAAABIze4LnWfn0sVcthgAAAAAACB/s3tTKquXLuayxQAAAAAAAPmf3U/fS7l0caVKlVSnTh0dPHhQM2bM0IABA9KM/afLFvv5+al8+fLpzpq6ffu2Tp48ab2dctniUqVKqVKlSpL+vGpMv3791KRJEzVr1kwzZ87kssUAAAAAAAC5xO5NqaxcupjLFgMAAAAAABQMJsPIpWsrg8sWo8Di6nuFm92vvscliwu3XIotfGbnD/yeUJCRrwo38hXsKhfyVWY/s+2+phQAAAAAAAAKH5pSAAAAAAAAsDmaUgAAAAAAALA5mlIAAAAAAACwOZpSAAAAAAAAsDmaUgAAAAAAALA5mlIAAAAAAACwOZpSAAAAAAAAsDmaUgAAAAAAALA5mlIAAAAAAACwOZpSAAAAAAAAsDmaUgAAAAAAALA5mlIAAAAAAACwOZpSAAAAAAAAsDmaUgAAAAAAALA5mlIAAAAAAACwOZpSAAAAAAAAsDmaUgAAAAAAALA5mlIAAAAAAACwOZpSAAAAAAAAsDmaUgAAAAAAALA5mlIAAAAAAACwOZpSAAAAAAAAsDmaUgAAAAAAALA5mlIAAAAAAACwOZpSAAAAAAAAsDmaUgAAAAAAALA5mlIAAAAAAACwOZpSAAAAAAAAsDmaUgAAAAAAALA5mlIAAAAAAACwOZpSAAAAAAAAsDmaUgAAAAAAALA5mlIAAAAAAACwOZpSAAAAAAAAsDmaUgAAAAAAALA5mlIAAAAAAACwOcesDLZYLNq+fbt27NihM2fOKD4+XqVLl1bDhg3l7++vihUr5ladAAAAeRo5CQAAIGsyNVPqzp07eu+991SxYkU9/fTT+u6773T9+nU5ODjo5MmTGjdunKpUqaKnn35au3fvzu2aAQAA8gxyEgAAQPZkaqbUI488ohYtWuizzz7Tk08+KScnpzRjzpw5o+XLl6tHjx4aM2aMBg0alOPFAgAA5DXkJAAAgOwxGYZh3G/QL7/8otq1a2fqgImJiTp79qyqVav2wMXldzdv3lSJEiV048YNFS9e3N7lADnGNMFk7xJgR8a4+35s5C4T779C7f6xJVse5DObnGQ7ZCsUZOSrwo18BbvKhXyV2c/sTJ2+l9mgJUlOTk4ELQAAUGiQk7InMDBQJUuWVNeuXe1dCgAAsJMsLXT+V0lJSfr000/1ww8/KDk5WY8//riCgoLk6uqak/UBAADkO+Sk+xs2bJgGDBigxYsX27sUAABgJ9luSg0dOlS//fabnn/+eSUmJmrJkiXat2+fVqxYkZP1AQAA5DvkpPtr06aNfvjhB3uXAQAA7ChTp+9J0tq1a1Pd3rx5szZt2qTXXntNw4YN07Jly/Tdd9/leIF5CdPMAQBAenIjJ926dUvDhw+Xt7e33Nzc9Nhjj2nv3r333e/ChQvq06ePHn74Ybm5ualevXrat29flh77fiIiItSxY0d5eXnJZDJp3bp1acaEhISocuXKcnV1VfPmzbVnz54crQEAAOR/mW5KLViwQJ07d1Z0dLQkqVGjRnr11Ve1ceNGffPNNxo1apSaNm2aa4XmBcOGDdOSJUvsXQYAAMhjciMnDRw4UFu2bFFoaKiOHDmi9u3by9/fXxcuXMhwn2vXrunxxx+Xk5OTvvvuOx07dkzTp09XyZIl0x2/a9cuJSYmptl+7NgxxcTEZPg4cXFx8vX1VUhISLr3h4WFKTg4WOPGjdOBAwfk6+urDh066PLly/d51gAAoDDJdFPqm2++Uc+ePdWmTRvNnj1b8+fPV/HixTVmzBi98847qlixopYvX56btdpdmzZt5O7ubu8yAABAHpPTOenOnTtavXq1pk2bplatWql69eoaP368qlevrrlz52a43/vvv6+KFStq4cKFatasmapUqaL27dunu7i6xWJRUFCQevXqpeTkZOv248ePy8/P7x/XegoICNB7772nwMDAdO+fMWOGBg0apP79+8vHx0fz5s1TkSJFtGDBgky/BgAAoODLdFNKkrp37649e/boyJEj6tChg/r06aP9+/fr0KFDCgkJUenSpbNcQOXKlWUymdL8BAUFZflYGcnMFHOJaeYAACD7cjInJSUlKTk5Oc3C6G5ubtq5c2eG+3399ddq0qSJunXrpjJlyqhhw4b67LPP0h1rNpu1YcMGHTx4UH379pXFYtGpU6fk5+enzp07a9SoUZmu968SEhK0f/9++fv7p3osf39/RUZGZuuYAACgYMpSU0qSHnroIc2fP18ffPCB+vbtq5EjR+ru3bvZLmDv3r26ePGi9WfLli2SpG7duqU7PjvTzO83xVximjkAAHhwOZWT3N3d1aJFC02cOFHR0dFKTk7W0qVLFRkZqYsXL2a43++//665c+eqRo0a2rRpk4YMGaKhQ4dmOOvJy8tL27Zt086dO9WrVy/5+fnJ39//H2dj3U9sbKySk5Pl6emZarunp6cuXbpkve3v769u3bppw4YNqlChwj82rEJCQuTj41Pgl4oAAKCwyXRT6uzZs3rhhRdUr1499e7dWzVq1ND+/ftVpEgR+fr6ZnuR89KlS6ts2bLWn2+//VbVqlVT69at04zN7jTz+00xl5hmDgAAsi83clJoaKgMw1D58uXl4uKiWbNmqWfPnjKbM45vFotFjRo10uTJk9WwYUMNHjxYgwYN0rx58zLcp1KlSgoNDVVYWJgcHR31+eefy2QyZbnerNq6dauuXLmi+Ph4nT9/Xi1atMhwbFBQkI4dO5aphd4BAED+kemmVN++fWU2m/XBBx+oTJkyeuWVV+Ts7KwJEyZo3bp1mjJlil544YUHKiYhIUFLly7VgAED0g1D+WWaOd/mAQBQuORGTqpWrZq2b9+u27dv69y5c9qzZ48SExNVtWrVDPcpV66cfHx8Um2rXbu2zp49m+E+MTExGjx4sDp27Kj4+HiNGDEiS3X+nYeHhxwcHNLMYI+JiVHZsmUf6NgAAKBgcczswH379unw4cOqVq2aOnTooCpVqljvq127tiIiIjR//vwHKmbdunW6fv26XnrppQzHpEwzb9mypXr16qXIyMhcnWb+66+/Wm/7+/vr8OHDiouLU4UKFbRq1ap0v9ULCgpSUFCQbt68qRIlSmS7LgAAkD/kZk4qWrSoihYtqmvXrmnTpk2aNm1ahmMff/xxHT9+PNW23377Td7e3umOj42NVbt27VS7dm2tWrVKv/32m9q0aSMXFxd9+OGH2arX2dlZjRs3Vnh4uDp37izpzxlc4eHhev3117N1TAAAUDBluinVuHFjjR07Vv369dPWrVtVr169NGMGDx78QMV8/vnnCggIkJeX1z+OS5lm3rp1a1WtWtWm08wBAAD+Ljdy0qZNm2QYhmrWrKmTJ09q5MiRqlWrlvr37y9JmjNnjtauXavw8HDrPiNGjNBjjz2myZMn64UXXtCePXs0f/78dBtiFotFAQEB8vb2tp665+Pjoy1btsjPz0/ly5fPcNbU7du3dfLkSevtqKgoHTp0SKVKlVKlSpUUHBysfv36qUmTJmrWrJlmzpypuLg4a+0AAABSFk7fW7Jkie7du6cRI0bowoUL+vTTT3O0kDNnzmjr1q0aOHDgfccyzRwAAOQluZGTbty4oaCgINWqVUt9+/bVE088oU2bNsnJyUnSn7OcTp06lWqfpk2bau3atVqxYoXq1q2riRMnaubMmerdu3ea45vNZk2ePFmrV6+Ws7Ozdbuvr6+2bt2a4UVnpD9nhjVs2FANGzaUJAUHB6thw4YaO3aspD+vRPjhhx9q7NixatCggQ4dOqSNGzemmZUOAAAKN5NhGIa9i5Ck8ePH69NPP9W5c+fk6JjxBK7Y2Fi1adNGNWrUSDXNvG/fvpmaZm4ymbR27VrrdPIUzZs3V7NmzTR79mxJf357WKlSJb3++ut66623svWcUk7fu3HjhooXL56tYwB5kWlC7s9MRN5ljLPzx4YNZsYiD8ul2MJndv7A7wkFGfmqcCNfwa5yIV9l9jM7U6fvxcXFqWjRopl+8KyOt1gsWrhwofr16/ePDansTjO/3xRzSUwzBwAA2ZLbOQkAAKCgytTpe9WrV9fUqVN18eLFDMcYhqEtW7YoICBAs2bNylIRW7du1dmzZzVgwIB/Ljab08zvN8VcYpo5AADIntzOSQAAAAVVpk7fO378uN5++22tX79evr6+atKkiby8vOTq6qpr167p2LFjioyMlKOjo0aPHq1XXnlFDg4Otqg/T2OKOQoqppcXbkwvh13lwdP3yEm2Q7ZCQUa+KtzIV7CrvH76Xs2aNbV69WqdPXtWq1at0o4dO/Tjjz/qzp078vDwUMOGDfXZZ58pICCAkAUAAAoVchIAAED25JmFzgsivs1DQcU3eYUb3+TBrvLgTCnYDr8nFGTkq8KNfAW7suNMqUytKQUAAAAAAADkJJpSAAAAAAAAsDmaUgAAAAAAALA5mlIAAAAAAACwOZpSAAAAAAAAsLksN6UqV66sd999V2fPns2NegAAAPItchIAAEDmZbkpNXz4cK1Zs0ZVq1bVk08+qZUrV+revXu5URsAAEC+Qk4CAADIvGw1pQ4dOqQ9e/aodu3aeuONN1SuXDm9/vrrOnDgQG7UCAAAkC+QkwAAADIv22tKNWrUSLNmzVJ0dLTGjRun//73v2ratKkaNGigBQsWyDCMnKwTAAAg3yAnAQAA3J9jdndMTEzU2rVrtXDhQm3ZskWPPvqoXn75ZZ0/f15vv/22tm7dquXLl+dkrQAAAPkCOQkAAOD+styUOnDggBYuXKgVK1bIbDarb9+++uijj1SrVi3rmMDAQDVt2jRHCwUAAMjryEkAAACZl+WmVNOmTfXkk09q7ty56ty5s5ycnNKMqVKlinr06JEjBQIAAOQX5CQAAIDMy3JT6vfff5e3t/c/jilatKgWLlyY7aIAAADyI3ISAABA5mV5ofPLly/rp59+SrP9p59+0r59+3KkKAAAgPyInAQAAJB5WW5KBQUF6dy5c2m2X7hwQUFBQTlSFAAAQH5ETgIAAMi8LDeljh07pkaNGqXZ3rBhQx07dixHigIAAMiPyEkAAACZl+WmlIuLi2JiYtJsv3jxohwds7xEFQAAQIFBTgIAAMi8LDel2rdvr9GjR+vGjRvWbdevX9fbb7+tJ598MkeLAwAAyE/ISQAAAJmX5a/sPvzwQ7Vq1Ure3t5q2LChJOnQoUPy9PRUaGhojhcIAACQX5CTAAAAMi/LTany5cvr559/1rJly3T48GG5ubmpf//+6tmzp5ycnHKjRgAAgHyBnAQAAJB52VrcoGjRoho8eHBO1wIAAJDvkZMAAAAyJ9srbh47dkxnz55VQkJCqu3PPffcAxcFAACQn5GTAAAA7i/LTanff/9dgYGBOnLkiEwmkwzDkCSZTCZJUnJycs5WCAAAkE+QkwAAADIvy1ffGzZsmKpUqaLLly+rSJEi+t///qeIiAg1adJEP/zwQy6UCAAAkD+QkwAAADIvyzOlIiMjtW3bNnl4eMhsNstsNuuJJ57QlClTNHToUB08eDA36gQAAMjzyEkAAACZl+WZUsnJyXJ3d5ckeXh4KDo6WpLk7e2t48eP52x1AAAA+Qg5CQAAIPOyPFOqbt26Onz4sKpUqaLmzZtr2rRpcnZ21vz581W1atXcqBEAACBfICcBAABkXpabUv/5z38UFxcnSXr33Xf17LPPqmXLlnr44YcVFhaW4wUCAADkF+QkAACAzMtyU6pDhw7Wf1evXl2//vqrrl69qpIlS1qvLAMAAFAYkZMAAAAyL0trSiUmJsrR0VFHjx5Ntb1UqVIELQAAUKiRkwAAALImS00pJycnVapUScnJyblVDwAAQL5ETgIAAMiaLF99b8yYMXr77bd19erV3KgHAAAg3yInAQAAZF6W15SaM2eOTp48KS8vL3l7e6to0aKp7j9w4ECOFQcAAJCfkJMAAAAyL8tNqc6dO+dCGQAAAPkfOQkAACDzstyUGjduXG7UAQAAkO+RkwAAADIvy2tKAQAAAAAAAA8qyzOlzGbzP17WmCvOAACAwoqcBAAAkHlZbkqtXbs21e3ExEQdPHhQixcv1oQJE3KsMAAAgPyGnAQAAJB5WW5KderUKc22rl27qk6dOgoLC9PLL7+cI4UBAADkN+QkAACAzMuxNaUeffRRhYeH59ThAAAACgxyEgAAQFo50pS6c+eOZs2apfLly+fE4QAAAAoMchIAAED6snz6XsmSJVMt4GkYhm7duqUiRYpo6dKlOVocAABAfkJOAgAAyLwsN6U++uijVGHLbDardOnSat68uUqWLJmjxQEAAOQn5CQAAIDMy3JT6qWXXsqFMgAAAPI/chIAAEDmZXlNqYULF2rVqlVptq9atUqLFy/OkaIAAADyI3ISAABA5mW5KTVlyhR5eHik2V6mTBlNnjw5R4oCAADIj8hJmRcYGKiSJUuqa9eu9i4FAADYSZabUmfPnlWVKlXSbPf29tbZs2dzpCgAAID8iJyUecOGDdOSJUvsXQYAALCjLDelypQpo59//jnN9sOHD+vhhx/OkaIAAADyI3JS5rVp00bu7u72LgMAANhRlptSPXv21NChQ/X9998rOTlZycnJ2rZtm4YNG6YePXrkRo0AAAD5Qk7mpFu3bmn48OHy9vaWm5ubHnvsMe3duzfT+0+dOlUmk0nDhw/P4rO4v4iICHXs2FFeXl4ymUxat25dmjEhISGqXLmyXF1d1bx5c+3ZsyfH6wAAAPlblptSEydOVPPmzdWuXTu5ubnJzc1N7du3l5+fH2slAACAQi0nc9LAgQO1ZcsWhYaG6siRI2rfvr38/f114cKF++67d+9effrpp6pfv/4/jtu1a5cSExPTbD927JhiYmIy3C8uLk6+vr4KCQlJ9/6wsDAFBwdr3LhxOnDggHx9fdWhQwddvnz5vrUDAIDCI8tNKWdnZ4WFhen48eNatmyZ1qxZo1OnTmnBggVydnbOjRrzDBbkBAAA/ySnctKdO3e0evVqTZs2Ta1atVL16tU1fvx4Va9eXXPnzv3HfW/fvq3evXvrs88+U8mSJTMcZ7FYFBQUpF69eik5Odm6/fjx4/Lz8/vHqwUGBATovffeU2BgYLr3z5gxQ4MGDVL//v3l4+OjefPmqUiRIlqwYMF9njkAAChMstyUSlGjRg1169ZNzz77rLy9vXOypjyLBTkBAEBmPGhOSkpKUnJyslxdXVNtd3Nz086dO/9x36CgID3zzDPy9/f/x3Fms1kbNmzQwYMH1bdvX1ksFp06dUp+fn7q3LmzRo0aleW6JSkhIUH79+9P9fhms1n+/v6KjIzM1jFDQkLk4+Ojpk2bZmt/AACQN2W5KdWlSxe9//77abZPmzZN3bp1y5Gi8ioW5AQAAP8kp3KSu7u7WrRooYkTJyo6OlrJyclaunSpIiMjdfHixQz3W7lypQ4cOKApU6Zk6nG8vLy0bds27dy5U7169ZKfn5/8/f3vOxvrn8TGxio5OVmenp6ptnt6eurSpUvW2/7+/urWrZs2bNigChUq/GPDKigoSMeOHcvSmloAACDvy3JTKiIiQk8//XSa7QEBAYqIiMhWERcuXFCfPn308MMPy83NTfXq1dO+ffuydaz0ZGYxTokFOQEAwIPJyZwUGhoqwzBUvnx5ubi4aNasWerZs6fM5vTj27lz5zRs2DAtW7YszQyrf1KpUiWFhoYqLCxMjo6O+vzzz2UymbJUa3Zs3bpVV65cUXx8vM6fP68WLVrk+mMCAIC8JctNqdu3b6e7JoKTk5Nu3ryZ5QKuXbumxx9/XE5OTvruu+907NgxTZ8+PcM1ELKzIOf9FuOUWJATAAA8uJzMSdWqVdP27dt1+/ZtnTt3Tnv27FFiYqKqVq2a7vj9+/fr8uXLatSokRwdHeXo6Kjt27dr1qxZcnR0TLVu1F/FxMRo8ODB6tixo+Lj4zVixIgs1fl3Hh4ecnBwSJPLYmJiVLZs2Qc6NgAAKFiy3JSqV6+ewsLC0mxfuXKlfHx8slzA+++/r4oVK2rhwoVq1qyZqlSpovbt26tatWppxmZ3Qc77LcYpsSAnAAB4cDmdkySpaNGiKleunK5du6ZNmzapU6dO6Y5r166djhw5okOHDll/mjRpot69e+vQoUNycHBIs09sbKzatWun2rVra82aNQoPD1dYWJjefPPNbNUq/bnYe+PGjRUeHm7dZrFYFB4ezmwoAACQimNWd3jnnXf0/PPPWxfClKTw8HCtWLFCq1atynIBX3/9tTp06KBu3bpp+/btKl++vF577TUNGjQozdiUBTlbtWqlvn37KjQ0VFFRUTm2IOfo0aNTPVZ2F+QMCQlRSEhIht9IAgCAgiknc9KmTZtkGIZq1qypkydPauTIkapVq5b69+8vSZozZ47Wrl1rbf64u7urbt26qY5RtGhRPfzww2m2S382igICAuTt7W09dc/Hx0dbtmyRn5+fypcvn+Gsqdu3b+vkyZPW21FRUTp06JBKlSqlSpUqKTg4WP369VOTJk3UrFkzzZw5U3FxcdbaAQAApGw0pTp27Kh169Zp8uTJ+vLLL+Xm5qb69etr69atat26dZYL+P333zV37lwFBwfr7bff1t69ezV06FA5OzurX79+acanLMjZsmVL9erVS5GRkbm6IOevv/5qve3v76/Dhw8rLi5OFSpU0KpVq9L9xi8oKEhBQUG6efOmSpQoke26AABA/pKTOenGjRsaPXq0zp8/r1KlSqlLly6aNGmSnJycJP2ZX06dOpXtWs1msyZPnqyWLVumOuXQ19dXW7duVenSpTPcd9++fWrbtq31dnBwsCSpX79+WrRokbp3764rV65o7NixunTpkho0aKCNGzemyVoAAKBwMxmGYeTUwY4ePZruN3H/xNnZWU2aNNGPP/5o3TZ06FDt3bv3H2cpRUREqHXr1qpataqOHz8uR8fM9ddMJpPWrl2rzp07W7dFR0erfPny+vHHH1M1mUaNGqXt27frp59+ytJzSpHSlLpx44aKFy+erWMAeZFpQu4vgIu8yxiXYx8b2WODBZiRh+VcbEnFFp/Z2clJSI1shYKMfFW4ka9gV7mQrzL7mZ3lNaX+7tatW5o/f76aNWsmX1/fLO9frly5NGss1K5dW2fPns1wHxbkBAAA+cGD5iQAAICCLNtNqYiICPXt21flypXThx9+KD8/P+3evTvLx3n88cd1/PjxVNt+++03eXt7pzueBTkBAEBel1M5CQAAoCDL0ppSly5d0qJFi/T555/r5s2beuGFF3Tv3j2tW7cu21eUGTFihB577DFNnjxZL7zwgvbs2aP58+dr/vz5acZmd0HO+y3GKYkFOQEAwAPJjZwEAABQkGW6KdWxY0dFRETomWee0cyZM/XUU0/JwcFB8+bNe6ACmjZtqrVr12r06NF69913VaVKFc2cOVO9e/dOMza7C3LebzFOSSzICQAAsi23chIAAEBBlumFzh0dHTV06FANGTJENWrUsG53cnLS4cOH+QYwHSzGiYKKhTgLNxbihF3l0YXOyUm2QbZCQUa+KtzIV7Cr/LDQ+c6dO3Xr1i01btxYzZs315w5cxQbG5sjxQIAAORn5CQAAICsy3RT6tFHH9Vnn32mixcv6pVXXtHKlSvl5eUli8WiLVu26NatW7lZJwAAQJ5FTgIAAMi6LF99r2jRohowYIB27typI0eO6F//+pemTp2qMmXK6LnnnsuNGgEAAPIFchIAAEDmZbkp9Vc1a9bUtGnTdP78ea1YsSKnagIAAMj3yEkAAAD/7IGaUikcHBzUuXNnff311zlxOAAAgAKDnAQAAJC+HGlKAQAAAAAAAFlBUwoAAAAAAAA2R1MKAAAAAAAANkdTCgAAAAAAADZHUwoAAAAAAAA2R1MKAAAAAAAANkdTCgAAAAAAADZHUwoAAAAAAAA2R1MKAAAAAAAANkdTCgAAAAAAADZHUwoAAAAAAAA2R1MKAAAAAAAANkdTCgAAAAAAADZHUwoAAAAAAAA2R1MKAAAAAAAANkdTCgAAAAAAADZHUwoAAAAAAAA2R1MKAAAAAAAANkdTCgAAAAAAADZHUwoAAAAAAAA2R1MKAAAAAAAANkdTCgAAAAAAADZHUwoAAAAAAAA2R1MKAAAAAAAANkdTCgAAAAAAADZHUwoAAAAAAAA2R1MKAAAAAAAANkdTCgAAAAAAADZHUwoAAAAAAAA2R1MKAAAAAAAANkdTCgAAAAAAADZHUwoAAAAAAAA2R1MKAAAAAAAANkdTCgAAAAAAADZHUwoAAAAAAAA2R1MKAAAAAAAANkdTCgAAAAAAADZHUwoAAAAAAAA2R1MKAAAAAAAANkdTCgAAAAAAADZHUwoAAAAAAAA2R1MKAAAAdhEYGKiSJUuqa9eu9i4FAADYAU0pAAAA2MWwYcO0ZMkSe5cBAADshKYUAAAA7KJNmzZyd3e3dxkAAMBOaEoBAADkUbdu3dLw4cPl7e0tNzc3PfbYY9q7d2+G46dMmaKmTZvK3d1dZcqUUefOnXX8+PEcrysiIkIdO3aUl5eXTCaT1q1bl+64kJAQVa5cWa6urmrevLn27NmT47UAAID8i6YUAABAHjVw4EBt2bJFoaGhOnLkiNq3by9/f39duHAh3fHbt29XUFCQdu/erS1btigxMVHt27dXXFxcho+xa9cuJSYmptl+7NgxxcTEpLtPXFycfH19FRISkuFxw8LCFBwcrHHjxunAgQPy9fVVhw4ddPny5fs8awAAUFjQlAIAAMiD7ty5o9WrV2vatGlq1aqVqlevrvHjx6t69eqaO3duuvts3LhRL730kurUqSNfX18tWrRIZ8+e1f79+9Mdb7FYFBQUpF69eik5Odm6/fjx4/Lz89PixYvT3S8gIEDvvfeeAgMDM6x/xowZGjRokPr37y8fHx/NmzdPRYoU0YIFC7LwKgAAgIKMphQAAEAelJSUpOTkZLm6uqba7ubmpp07d2bqGDdu3JAklSpVKt37zWazNmzYoIMHD6pv376yWCw6deqU/Pz81LlzZ40aNSpbtSckJGj//v3y9/dP9Vj+/v6KjIzM8vFCQkLk4+Ojpk2bZqseAACQN9GUAgAAyIPc3d3VokULTZw4UdHR0UpOTtbSpUsVGRmpixcv3nd/i8Wi4cOH6/HHH1fdunUzHOfl5aVt27Zp586d6tWrl/z8/OTv75/hbKzMiI2NVXJysjw9PVNt9/T01KVLl6y3/f391a1bN23YsEEVKlTIsGEVFBSkY8eO/eN6WgAAIP9xtHcBAAAASF9oaKgGDBig8uXLy8HBQY0aNVLPnj0zPB3vr4KCgnT06NFMzaqqVKmSQkND1bp1a1WtWlWff/65TCZTTjyFf7R169ZcfwwAAJB3MVMKAAAgj6pWrZq2b9+u27dv69y5c9qzZ48SExNVtWrVf9zv9ddf17fffqvvv/9eFSpUuO/jxMTEaPDgwerYsaPi4+M1YsSIB6rbw8NDDg4OaRZKj4mJUdmyZR/o2AAAoOCgKQUAAJDHFS1aVOXKldO1a9e0adMmderUKd1xhmHo9ddf19q1a7Vt2zZVqVLlvseOjY1Vu3btVLt2ba1Zs0bh4eEKCwvTm2++me16nZ2d1bhxY4WHh1u3WSwWhYeHq0WLFtk+LgAAKFg4fQ8AACCP2rRpkwzDUM2aNXXy5EmNHDlStWrVUv/+/SVJc+bM0dq1a63Nn6CgIC1fvlxfffWV3N3dres3lShRQm5ubmmOb7FYFBAQIG9vb4WFhcnR0VE+Pj7asmWL/Pz8VL58+XRnTd2+fVsnT5603o6KitKhQ4dUqlQpVapUSZIUHBysfv36qUmTJmrWrJlmzpypuLg4a+0AAAA0pQAAAPKoGzduaPTo0Tp//rxKlSqlLl26aNKkSXJycpL05yynU6dOWcenLE7epk2bVMdZuHChXnrppTTHN5vNmjx5slq2bClnZ2frdl9fX23dulWlS5dOt659+/apbdu21tvBwcGSpH79+mnRokWSpO7du+vKlSsaO3asLl26pAYNGmjjxo1pFj8HAACFl8kwDMPeRRRUN2/eVIkSJXTjxg0VL17c3uUAOcY0IfcXv0XeZYyz88eGDRZfRh6WS7GFz+z8gd8TCjLyVeFGvoJd5UK+yuxnNmtKAQAAAAAAwOZoSgEAAAAAAMDmaEoBAAAAAADA5mhKAQAAAAAAwOZoSgEAAAAAAMDmaEplQWBgoEqWLKmuXbvauxQAAAAAAIB8jaZUFgwbNkxLliyxdxkAAAAAAAD5Hk2pLGjTpo3c3d3tXQYAAAAAAEC+Z/em1Pjx42UymVL91KpVK0cfIyIiQh07dpSXl5dMJpPWrVuX7riQkBBVrlxZrq6uat68ufbs2ZOjdQAAAAAAAOBPdm9KSVKdOnV08eJF68/OnTszHLtr1y4lJiam2X7s2DHFxMSku09cXJx8fX0VEhKS4XHDwsIUHByscePG6cCBA/L19VWHDh10+fLlrD8hAAAAAAAA/KM80ZRydHRU2bJlrT8eHh7pjrNYLAoKClKvXr2UnJxs3X78+HH5+flp8eLF6e4XEBCg9957T4GBgRnWMGPGDA0aNEj9+/eXj4+P5s2bpyJFimjBggUP9uQAAAAAAACQRp5oSp04cUJeXl6qWrWqevfurbNnz6Y7zmw2a8OGDTp48KD69u0ri8WiU6dOyc/PT507d9aoUaOy9fgJCQnav3+//P39Uz2Wv7+/IiMjs3y8kJAQ+fj4qGnTptmqBwAAAAAAoKCze1OqefPmWrRokTZu3Ki5c+cqKipKLVu21K1bt9Id7+XlpW3btmnnzp3q1auX/Pz85O/vr7lz52a7htjYWCUnJ8vT0zPVdk9PT126dMl629/fX926ddOGDRtUoUKFDBtWQUFBOnbsmPbu3ZvtmgAAAAAAAAoyR3sXEBAQYP13/fr11bx5c3l7e+uLL77Qyy+/nO4+lSpVUmhoqFq3bq2qVavq888/l8lkyvVat27dmuuPAQAAAAAAUBjYfabU3z300EN65JFHdPLkyQzHxMTEaPDgwerYsaPi4+M1YsSIB3pMDw8POTg4pFkoPSYmRmXLln2gYwMAAAAAACCtPNeUun37tk6dOqVy5cqle39sbKzatWun2rVra82aNQoPD1dYWJjefPPNbD+ms7OzGjdurPDwcOs2i8Wi8PBwtWjRItvHBQAAAAAAQPrsfvrem2++qY4dO8rb21vR0dEaN26cHBwc1LNnzzRjLRaLAgIC5O3trbCwMDk6OsrHx0dbtmyRn5+fypcvn+6sqdu3b6eaeRUVFaVDhw6pVKlSqlSpkiQpODhY/fr1U5MmTdSsWTPNnDlTcXFx6t+/f+49eQAAAAAAgELK7k2p8+fPq2fPnvrjjz9UunRpPfHEE9q9e7dKly6dZqzZbNbkyZPVsmVLOTs7W7f7+vpq69at6e4jSfv27VPbtm2tt4ODgyVJ/fr106JFiyRJ3bt315UrVzR27FhdunRJDRo00MaNG9Msfg4AAAAAAIAHZzIMw7B3EQXVzZs3VaJECd24cUPFixe3dzlAjjFNyP0LCyDvMsbZ+WPDBhe2QB6WS7GFz+z8gd8TCjLyVeFGvoJd5UK+yuxndp5bUwoAAAAAAAAFH00pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANgcTSkAAAAAAADYHE0pAAAAAAAA2BxNKQAAAAAAANico70LQPaYTPauAPZkGPauAACAgoVsBfIVANgeM6UAAAAAAABgczSlAAAAAAAAYHM0pQAAAAAAAGBzNKUAAABgF4GBgSpZsqS6du1q71IAAIAd0JQCAACAXQwbNkxLliyxdxkAAMBOaEoBAADALtq0aSN3d3d7lwEAAOyEphQAAEABcuvWLQ0fPlze3t5yc3PTY489pr179+boY0RERKhjx47y8vKSyWTSunXr0h0XEhKiypUry9XVVc2bN9eePXtytA4AAJC/0ZQCAAAoQAYOHKgtW7YoNDRUR44cUfv27eXv768LFy6kO37Xrl1KTExMs/3YsWOKiYlJd5+4uDj5+voqJCQkwzrCwsIUHByscePG6cCBA/L19VWHDh10+fLl7D0xAABQ4NCUAgAAKCDu3Lmj1atXa9q0aWrVqpWqV6+u8ePHq3r16po7d26a8RaLRUFBQerVq5eSk5Ot248fPy4/Pz8tXrw43ccJCAjQe++9p8DAwAxrmTFjhgYNGqT+/fvLx8dH8+bNU5EiRbRgwYIHf6IAAKBAoCkFAABQQCQlJSk5OVmurq6ptru5uWnnzp1pxpvNZm3YsEEHDx5U3759ZbFYdOrUKfn5+alz584aNWpUtupISEjQ/v375e/vn+qx/P39FRkZmeXjhYSEyMfHR02bNs1WPQAAIG+iKQUAAFBAuLu7q0WLFpo4caKio6OVnJyspUuXKjIyUhcvXkx3Hy8vL23btk07d+5Ur1695OfnJ39//3RnVmVWbGyskpOT5enpmWq7p6enLl26ZL3t7++vbt26acOGDapQoUKGDaugoCAdO3Ysx9fGAgAA9uVo7wIAAACQc0JDQzVgwACVL19eDg4OatSokXr27Kn9+/dnuE+lSpUUGhqq1q1bq2rVqvr8889lMplyvdatW7fm+mMAAIC8i5lSAAAABUi1atW0fft23b59W+fOndOePXuUmJioqlWrZrhPTEyMBg8erI4dOyo+Pl4jRox4oBo8PDzk4OCQZqH0mJgYlS1b9oGODQAACg5mSuUiwzAkSTdv3rRzJSho7P6Wumvnx4dd8TcNdpVL77+U93XKZ3dBULRoURUtWlTXrl3Tpk2bNG3atHTHxcbGql27dqpdu7ZWrVql3377TW3atJGLi4s+/PDDbD22s7OzGjdurPDwcHXu3FnSn4uqh4eH6/XXX8/uUyJbIVfZ/W1FvirU+LsGu8qF919msxVNqVx069YtSVLFihXtXAkKmhIl7F0BCrMSU3kDwo5y+Q/grVu3VCKf/5HdtGmTDMNQzZo1dfLkSY0cOVK1atVS//7904y1WCwKCAiQt7e3wsLC5OjoKB8fH23ZskV+fn4qX758urOmbt++rZMnT1pvR0VF6dChQypVqpQqVaokSQoODla/fv3UpEkTNWvWTDNnzlRcXFy6dWQW2Qq5KZ//Xx/5HPkKdpWLfwDvl61MRkH6SjCPsVgsio6Olru7u03WZSgsbt68qYoVK+rcuXMqXry4vctBIcP7D/bE+y/3GIahW7duycvLS2Zz/l7d4IsvvtDo0aN1/vx5lSpVSl26dNGkSZMyDIRbtmxRy5Yt01yx7+DBgypdurQqVKiQZp8ffvhBbdu2TbO9X79+WrRokfX2nDlz9MEHH+jSpUtq0KCBZs2apebNm2f7uZGtcg9/X2BPvP9gT7z/ckdmsxVNKeQ7N2/eVIkSJXTjxg3+aMDmeP/Bnnj/Acgt/H2BPfH+gz3x/rOv/P1VIAAAAAAAAPIlmlIAAAAAAACwOZpSyHdcXFw0btw4ubi42LsUFEK8/2BPvP8A5Bb+vsCeeP/Bnnj/2RdrSgEAAAAAAMDmmCkFAAAAAAAAm6MpBQAAAAAAAJujKQUAAAAAAACboykFAAAAAAAAm6MpBTyglGsFXLhwwc6VAAAA5H9kKwAoPGhKAQ/IZDJp7dq16tmzp3777Td7lwMAkiSLxWLvEgAgW8hWAPIislXucLR3AUB+ZRiGTCaTzpw5ow8//FD9+/fXI488Yu+yAOt702KxyGzmu4fCKOV3f+nSJR0/flwWi0WNGzdW8eLF7V0aAGSIbIW8imwFslXuoSkFZJPJZNLOnTv13XffqVy5curSpYu9SwKsoen777/X119/rZIlSyogIEBNmza1d2mwkZTQdOTIEXXr1k2Ojo46duyYAgMDNWbMGDVq1MjeJQJAushWyIvIViBb5S7avMAD2Lp1q6ZMmaKIiAjFxMTYuxxAJpNJmzZt0pNPPqkzZ85o9uzZGjVqlObNm2fv0mADKaHp559/1qOPPqouXbroq6++0qpVq/TVV1/pm2++sXeJAPCPyFbIa8hWhRvZKvfRlAIewPjx4zV9+nQlJCRowYIFio6OtndJKOTOnz+vjRs3KiQkRGvWrNHhw4dVoUIFLVu2TCEhIfYuD7nMbDbr119/VaNGjfTKK69o0qRJqlatmp599ll5e3tr8+bNSkhISLVPyoLCAJAXkK2Q15CtCjeyVe6jKQVkUsofl1OnTunnn3/Wjh07JEkjRozQyJEjtWLFCi1cuFCXLl2yZ5koxPbt26chQ4Zox44dql+/viTJy8tLU6dOVdWqVbVixQrNnTvXzlUit0VFRclisah48eK6evWqJOmjjz5SVFSU7ty5o+HDh+vDDz/Uhg0bJLFoJwD7IVshryNbQSJb5TbWlAIyIeVc8rVr1+qdd95RUlKSpD8/lFavXq0xY8bIYrFo3rx5cnBwUN++feXl5WXnqlHYFClSRDdu3NCxY8f0008/qUWLFpKk8uXLa8qUKfrPf/6jkJAQOTk5aeDAgXauFrklICBAS5cuVZ8+feTs7Kx79+5p9uzZWrp0qUqVKqULFy5o5cqV+uSTT2QymfTSSy9pzJgxLNwKwKbIVsgPyFaQyFa5zgCQKdu2bTOKFi1qfPbZZ8aNGzeMjRs3GiaTyViwYIF1zMSJE40iRYoY06dPN5KSkuxYLQqr48ePGx06dDBatmxprFq1KtV9586dM4YMGWJERUXZpzjkOovFYv13aGioYTKZDJPJZKxZsybVuOvXrxsnT540Xn/9deO3336zdZkAYBgG2Qr5A9mqcCNb5T6TYXDCI/B3N27cUIkSJVJd9nXy5MmKiYnRxx9/rNOnT6tt27YKCAjQJ598kmrf999/X88//7xq1Khhj9JRSBj//xvm//3vfzp9+rRcXFxUt25dlS1bVr/88ouGDx8ui8WiV155RV27drXul5ycLAcHBztWjpyS8h74p+3r1q3T888/rzFjxmjYsGHy8PCQJC5pDcDmyFbI68hWIFvZB68a8DfLly+Xp6enfv/9d5nNZus5wfv375dhGLp+/bpatmyp9u3bWxc3XLBggTVA/fvf/yY0IdeZTCZ9+eWXevLJJzV06FANGjRI9erVU3h4uGrXrq2PPvpIZrNZn3/+uZYtW2bdj9BUMKSEo8uXL+vMmTOp7jOZTNZ1Wjp37qwlS5Zo0qRJmj59uv744w9JIjQBsCmyFfIDslXhRrayH1454G8effRRtWjRQn5+foqKirL+genSpYtOnTqlmjVr6umnn9ann34q6c9vR/bt26fffvtNd+/etWfpKET279+vl19+We+++6527typb7/9Vs8995yee+45/fDDD/Lx8dGMGTN07do1rV69Wrdu3bJ3ychBJpNJ169fV/fu3TVhwgRFRUWlud8wDBmGoT59+ig0NFTTp0/X+PHjde3aNTtVDaCwIlshPyBbFW5kK/uhKQX8TZUqVbRs2TLVqVNHLVu2tHbKa9asqcuXL6tkyZLq06ePJOn27dsaN26c1q1bpyFDhsjV1dWepaOA+uabb6wLwKY4ceKE6tSpoz59+qhcuXKqU6eOPv30U3Xr1k19+vTRlStXVKdOHYWGhmrmzJlyd3e3U/XISX894/6hhx6Sv7+/jh49qpkzZ+r3339PNfav08979+6tzz//XEuXLlViYqLN6gUAiWyFvIdshRRkK/ujKQX8hcVikclk0oULF9SjRw9FR0frqaeeUlRUlBo3bqypU6fK0dFRQ4cOVaNGjdStWzd9/vnnWr9+vWrWrGnv8lEAHTp0SH379tXly5dTbb9586YOHz5s/SBNTk6Wo6OjhgwZIrPZbP12p0aNGqpUqZLN60bOS/n7FB8fr3PnzkmSxowZo969e2vHjh36+OOPreEp5X1hMpl07949ffXVV+rTp4/OnTunMmXK2O05ACh8yFbIa8hWSEG2yhtoSgF/YTabtXr1agUEBOjnn39WYGCg7t69q5YtW+rUqVNq166dQkND9e9//1tPPPGEevTooV27dqlhw4b2Lh0FVIMGDRQVFSUvLy/9+uuvSkhIkCT5+/vrkUce0YQJE3T9+nXregalS5eWk5OTdRwKhpTFM3/99Ve9+OKL6tSpk95//31J0rBhw9S3b99U4Snlm7yEhAQNHTpUQ4YM0cWLF1WsWDF7Pg0AhRDZCnkN2QoS2SpPsd2F/oC878qVK0atWrWM9957z7rt4MGDRuvWrY3y5csbv//+ux2rQ2FisVisP4ZhGJcuXTLMZrPxxhtvGMnJyUZSUpIxZswY4/HHHzf+9a9/GVevXjViY2ONMWPGGFWrVjWio6Pt/AyQU5KTkw3DMIyff/7Z8PT0NN566y1j+/btxr1791KN++ijj4yGDRsaQ4cOtf6tCgoKMooWLWrs3bvX5nUDgGGQrZB3kK2QgmyVtzjauykG5CX37t3TzZs31aBBA+u2evXqacaMGXruuefUqVMnrV27VtWqVbNfkSjQUr61uXfvnnUdjaioKFWuXFmhoaEaNGiQnJycNH36dI0bN05ms1nr16/XzJkz5evrq+joaK1fv17lypWz8zNBTjGbzYqOjtYLL7ygHj16aMqUKdb7Uq5gZTabNXz4cJlMJi1evFiSdOXKFX311VfasWOHGjVqZJfaAYBsBXsjW+HvyFZ5jL27YkBe06JFC2PAgAGptiUnJxtPPfWUYTKZjFq1ahkJCQl2qg6FwdmzZ43evXsb0dHRxrp164wSJUoYx48fNwzDMMLCwgxHR0cjODjYMIw/35uXLl0yVqxYYWzatMk4c+aMPUtHLlm5cqXRpEkTIyoqKt37ExMTrf/++OOPjdKlSxvFixc3Dhw4YKMKASBjZCvYG9kKf0e2yjuYKYVCyzAM66U9UxYylKSuXbtq5cqVmjVrloYOHSrpz055pUqVtGbNGjVr1kxOTk72LB0F3L59+3TmzBk9//zzOnjwoBYsWKBHHnlEhmHohRdekPTnFT9MJpPef/99eXp6qkePHnauGrkpMjJSiYmJqly5cpr7DMOQo6Oj7ty5Izc3Nw0dOlTu7u564oknVKNGDdsXC6DQIlshryJb4e/IVnkHTSkUSimhafPmzfriiy904sQJdejQQc8884xef/11nTx5UkuWLNHu3bv1zDPPKCIiQl9//bVGjx4tLy8ve5ePAi4wMFCHDx/Wu+++q4YNG+rRRx+13pcSnkwmk1566SXduXNHM2fOJMwXcCVLllRMTIxiY2Pl4eGR6r6UhTdfffVVPfLIIxozZoz69+9vjzIBFGJkK+RlZCv8Hdkq7+DqeyiUTCaTvvrqKz333HMyDEM1a9bUggULNGLECG3dulUzZ87UgAEDFB0drYkTJ+rQoUP67rvv0u2kAznF+P+Xmk1KSlLZsmX19ttvy8PDQyNGjNDhw4et3z4bhqFu3bpp/vz5WrVqla5du2bnypHbqlevrlu3bmn58uWKj4+X9H9rHkh/XgnG0dFRFStWtFeJAAo5shXyIrIVMkK2yjtMRsr/U4FC5MqVK+rYsaO6deumf/3rX5KkI0eOaOrUqbp48aJCQkJUu3ZtSdIff/whV1dXFS1a1J4lo4BL+YZ5y5YtOnr0qJ5//nl5e3srLCxM8+fPV7FixTRx4kTVr19fknTw4EE1aNBAt2/flru7u52rhy34+fnpyJEjmj59ujp37qzixYtLkpKTkzVhwgStWrWK/8ADYDdkK+Q1ZCvcD9kqb2CmFAolV1dXXbt2zfqHxzAM1atXT6NHj9bJkye1ZcsW69iHH36Y0IRcZzKZtGbNGnXp0kXnzp1TXFycJKl79+565ZVXFB8fr//85z/avn27JkyYoPbt2+vq1auEpgIo5bui48eP65dfftEvv/wiSVq3bp2qVq2qN954Q2+88Ya2b9+uuXPn6tVXX9WsWbO0YsUKQhMAuyFbIa8hWyEF2SpvY00pFAop35QkJibKyclJd+/elaurqy5evCjpz264g4OD6tatq+bNmysiIkJvvPGG9XxiILf973//0xtvvKGPPvpIL7/8cqr7XnjhBTk6Omr+/Pnq1auXXFxctH79ej388MN2qha5KSVEv/rqq3JxcZGHh4cGDhyooKAg7d69WwMGDNCuXbsUGhqqatWqqWHDhtq1a5fq1Klj79IBFCJkK+R1ZCukIFvlbTSlUOD9deHNb775Rv/+979VoUIFDRo0SMOGDZOvr686depkHX/v3j3VrFmT0IRcs3HjRrVv315m8/9NVo2Ojpanp6eeffZZa5C3WCzWMc8//7yaNWumy5cvy9PTU+XLl7dX+chFhmHojz/+0OTJk/XBBx+obNmy2rFjh9555x3duHFDb7/9thYuXKhr167p8uXLqlSpkiTJzc3NzpUDKEzIVshryFbICNkq76MphQIvpTPev39/DRw4UFeuXFGFChX0+uuv68yZMwoMDNSoUaNUqlQpXbx4UT/88IPef/99e5eNAmrPnj3q3r27jh8/rrJly1q3nzlzRr/++qtKliwpBwcHa3iSpAMHDsjT01MVKlRQhQoV7FU6clHKf+ClLLBZv359denSRcWKFVPDhg1VtGhRTZ06VSaTSaNHj1bJkiXl7u5uvdw6ANgS2Qp5CdkK6SFb5R+84ijwTpw4oREjRmjy5MkKCgpKdd8HH3ygGjVqaMmSJbpz5448PDy0Y8cO60KcQE5r1qyZfv/9dz388MM6ceKEqlatKgcHB7Vq1Ure3t5699139dZbb6lYsWLW8DR79mzVq1dPI0aM4FvmAiglNH377bf67LPPVLRoUR07dsx6f5kyZTRw4EBJ0vTp03Xv3j2NHz+e0ATAbshWyEvIVvg7slX+wkLnKPBiYmJUokQJBQYGWhe5S05Ott4/ePBgrV+/Xrt379aaNWvk6+trr1JRgH366afauXOnLBaLHn74YZ0/f141a9bU22+/LUmqXLmy2rdvr++//17vvvuubty4oaioKP3nP//Rhg0b9PTTTxOaCpiUv0cmk0k//PCDevbsqSJFiig+Pl5Hjx7VBx98YB1bunRpDRw4UEOGDNHChQv1xx9/iIvnArAXshXyArIV/o5slT+ZDF55FHAbNmxQYGCgjhw5okceecTaOZekXbt2ydvbm2m7yHVVqlSRo6Ojli5dqqZNm8psNuu///2v3njjDQUHB2vSpEm6c+eOJk2apG+//VZHjx5VrVq1FB8fr9WrV6thw4b2fgrIJadPn9b27dv1xx9/KDg4WNevX9fKlSv1xhtvaOzYsXrnnXesY2NjY2UymViIFYBdka2QF5CtkBGyVT5jAAWIxWJJs+2XX34x6tevb7z11lvGhQsXUo3r27evMWrUKCM5OdmmdaLwSHmvJScnG02aNDFq1qxp/Pjjj0ZiYqJhGIaxaNEiw8HBwRg9erRhGIaRmJhoxMbGGmvXrjUiIyOt71kUDBMnTjQiIyOtty9evGiYTCbD1dXVmDx5snX73bt3jblz5xoODg7GpEmT7FEqABiGQbZC3kO2wl+RrfI/TppEgWH8/2/pdu/erd9++00Wi0UvvfSSatWqpeeff17Lly9XYmKi+vTpIxcXFy1cuFAbNmxQREREqit1ADnJZDLp3r17cnFxUUREhBo2bKh///vfmjp1qpo3b65+/fpJkl5++WWZTCZNnDhRDz/8sDp37mzfwpGjUv4+bdu2TYGBgdbtZcuW1cqVKzV48GAdPXpU8fHxKlKkiFxcXNS/f3+ZzWa9+uqrcnZ21ptvvmnHZwCgMCJbIS8iW0EiWxUo9u2JATlr7dq1hqurq9GwYUPDycnJ8PPzM6Kjow3DMIxp06YZLVq0MEwmk1GnTh2jRo0axoEDB+xcMQq6lG/zVq5caQwcONDw8/MzTCaTUb9+fSMyMtL6TfKiRYsMV1dXIzg42EhKSrJnychh6c0W2LZtm7Fjxw7r7zosLMxwdHQ0/v3vfxv37t2zjrt7966xYMEC49ixYzarFwD+imyFvIZsBbJVwcKaUsj3LBaLzGazbt++rZ49e6pbt2567rnndPnyZT399NMqVaqUVq9erYoVK+rq1av69ddfVbRoUZUrV05lypSxd/koBHbs2KEOHTpo9uzZ8vX1VVJSkgYNGqSkpCQtXLhQTZs2lYODgz799FONGTNGv/zyi0qXLm3vspEDUv4+nT59Wps2bVKDBg3UvHlzNWvWTGfOnNHatWvVvHlzOTg4aOXKlXrxxRf1r3/9SxMnTpSTk5O9ywdQSJGtkNeRrQovslXBQ1MK+U7KH6KEhAQ5OztLksLDw/XRRx/J2dlZH3zwgapVqyZJunTpkp544gk9/PDDWrFihapWrWrP0lFIffzxx1q+fLl27Nhhfc/Gx8erWbNmcnJy0ieffKKmTZvK0dFRN27cUIkSJexcMXJCyt+qI0eOqGvXrqpTp44GDBigZ599VpL0xBNP6NKlS1q0aJFatGhhDU8DBgzQgAED9NFHHxGeANgE2Qr5DdmqcCJbFUyc7I18JeUP0dGjRzV8+HBdvXpVkvTQQw9p9+7dWr9+vW7dumUdW7ZsWe3atUs3b97Us88+q99//92e5aOQSen537hxQ9euXbOGpjt37qhIkSKaOXOmDh8+rJdfflkHDx6UJEJTAWI2m/Xrr7+qdevWev755zVnzhxraJKknTt3ytPTU3369FFkZKSSk5PVo0cPzZ07V2FhYbp27ZodqwdQWJCtkJ+QrQo3slXBRFMK+UZKaDp8+LDq16+vcuXKqVSpUpKkxo0bKzw8XCVLltTYsWN1/fp1mc1mGYYhT09Pbdu2TW5ubiy6CZtKuTx29+7dFRMTowkTJkiS3NzcJEnOzs7q3LmzHnroIet7GQXH3bt3NXbsWPXq1UtTpkyRl5eXJCkxMVFRUVG6cuWKdu3apdq1a6t379766aeflJycrH79+unUqVOcAgMg15GtkN+QrQo3slXBxKcI8oWU0HTo0CG1aNFCo0eP1jvvvGO9/+7du/L19dX69esVGRmpAQMG6Pr16zKZTLJYLCpXrpz27NmjypUr2+9JoMBL+fbu4MGDWrRokRYtWqTdu3erZs2aevvttxUaGqpx48ZJkm7evKnNmzfLy8tLERER1tMiUHA4Ojrq0qVLqlWrlnXbpk2bNGrUKDVo0ECNGjVSt27d9N1338nHx0dPP/209u7dK0kqXry4vcoGUEiQrZAfkK3wV2SrgsnR3gUA92MYhsxms/73v//pscceSxOa5s6dK3d3d/Xo0UONGzfWxo0b9dRTT2nw4MH69NNPVbJkSUmSg4ODvZ4CCgmTyaTVq1fr9ddf1yOPPCI3NzdFRERo4cKFGjx4sBwdHTVp0iQtXLhQxYsX14ULFxQeHi5HR/4UF0Tx8fG6cuWKfv75Zx0/flxr1qzR4sWLVbduXU2cOFHFihXTu+++q/fee0/fffed/P395eHhYe+yARQCZCvkF2Qr/BXZqoCyz0X/gKy5du2aUadOHaNu3bpGbGysdfvUqVMNR0dHIyIiItX4/fv3GyaTyejTp4/1srFAbjt06JBRunRpY+7cuYZhGMbBgwcNk8lkjBgxwjAMw7h3755x5swZY9asWcaiRYuMEydO2LNc2EB4eLjh6OhoeHt7G+7u7sa8efOsv/eEhASjffv2Rs+ePe1cJYDCiGyF/IBshb8jWxU8tJCRLzz00EPq1q2bNmzYoKlTp2rKlCmaM2eOpk2bpg0bNqhly5YyDMN6nnmjRo20f/9+FSlSxLoNyG1RUVFq3LixXn31VZ0+fVrPPfechgwZohkzZkiSzp07p2rVqumNN96wc6WwFT8/P/3++++6fPmyvL29U31b5+DgoBIlSqhatWqyWCySxNosAGyGbIX8gGyFvyNbFTw0pZDnpax5MG7cODk7O2v16tXy8/PTzz//rPXr1+vxxx9PFZoWL16sBg0aqGHDhnauHAVdyvtu3759KlWqlCwWi+7evasjR47o2WefVUBAgGbPni1J2r59u7788ku98847LLJYyFSsWFEVK1ZMtS0hIUETJ07Url27NGnSJAITAJsiWyGvIlshM8hWBQu/KeR5ZrPZ2ukePXq0unfvrqioKLVv316PPPJIqrHvvPOO3njjDRUpUsQepaKQMZlM+u6779ShQwedPHlSZcqU0fXr19W2bVv5+/vr008/tX4grl27VhcvXpSrq6udq4a9LV26VCNHjtRnn32mb7/9VjVq1LB3SQAKGbIV8iqyFbKDbJW/MVMK+UJKeDKbzRo5cqQMw9DKlSs1depUDR8+XBUrVtTYsWP14YcfaseOHfwhQq5K+Rbv6tWr2rx5s0aPHq327dtLkgICAjR16lQ1adJEp0+flqOjo2bPnq2lS5dq+/btXPmjkDt+/Lg+//xzlSxZUt9//71q165t75IAFFJkK+QlZCtkF9kq/zMZxv+/ziZgZ3+dJp6cnJzuFV1SwpMkTZ06VV988YWeeeYZXb9+Xf/973+1c+dONW7c2KZ1o+D76/suxf79+/XUU0+pXLlymjhxojp16mS9b/DgwYqIiND58+dVt25dXbt2TStXruS0B0iSLl++LBcXF5UoUcLepQAo4MhWyKvIVshJZKv8jaYU8oSU0PTHH3/IbDarZMmS2rhxo6pUqaKaNWumGvvXD7Fp06bpww8/1N27d/XDDz+oUaNG9igfBVjK+y06OlqHDh3StWvX1KFDB3l4eKhbt25avXq1Jk+erODgYDk7O1v3O3z4sM6cOSNPT095e3urbNmydnwWAIDChmyFvIpsBeCvaEohz7h8+bJ69uyp9u3bq2zZsurfv7/WrFmjzp07pxn71/A0b948+fn5pVkDAXhQKe+zn3/+WYGBgTKbzTp16pRq1aqlkJAQtW3bVoGBgdq+fbuWL18uf39/OTpyVjQAIG8gWyGvIVsB+DuaUshTgoOD9c033+j06dOaPXu2Xn311XSn90oZT0MHckLK++7IkSNq3ry5Ro4cqZ49e8pisSgwMFDu7u7at2+fJOnpp5/WgQMHtHjxYrVr147wBADIM8hWyCvIVgDSw9X3kCckJydLknr27KkrV67Iy8tLt2/f1rVr11JdIeavCE3ITWazWVFRUfL19VXfvn01YcIE1apVSz4+Ppo8ebL+97//ae/evZKkDRs2qHHjxho4cKA2btyopKQkO1cPACjsyFbIa8hWANJDUwp5goODg06ePKly5cpp/fr16tGjh1auXKk5c+bo+vXrGYYnIDe5urrKyclJFy9e1G+//abExERJf67TUaxYMbm7u1tD//r16+Xt7a0333xT9+7ds2fZAACQrZAnka0A/B2n7yFP+P3339WoUSPt2rVLderUkfTndPOIiAg9//zzeu211/TQQw9p7ty5ateuHWscINelnMJw5swZNWnSRL6+vvriiy9069YtNWvWTP3799fUqVMlSUlJSdZp5efOnVPFihXtWToAAGQr5DlkKwDpoSmFPOGXX37RE088oe+//17169e3bg8ODtauXbtUs2ZNlSxZUrNnz9bRo0fl4+Njx2pRWKSEp9OnT6tJkyaqWbOmzp07p2eeeUZz586V9H/rI7AOBwAgLyFbIS8iWwH4O07fg138vRdau3Zt1ahRQ5GRkZKkO3fuSJJmzJihjh076vbt29q3b58OHTpEaILNODg4KCkpSZUrV9b+/ft1/vx53bp1S8OGDbOOMZlM1rEAANgL2Qr5AdkKwN9xGQPYhclk0vr167Vx40aVK1dOzZs31x9//KFLly5Jktzc3Kxj//Of/yghIUEJCQkqVqyYvUpGIeXo6KikpCR5e3tr586datSokYYNG6Y5c+aoRo0a1uAEAIA9ka2QX5CtAPwVp+/BLhITExUSEqLNmzfr/PnzKlKkiA4ePKjExET16tVLLi4uevLJJ5WUlKTnn39eRYoUsXfJKORS1jY4ffq0HnvsMVWsWFHLly9XtWrV7F0aAABkK+Q7ZCsAEk0p2JBhGBl+83Ht2jV99tlnmjdvnvz8/BQbG6vTp08rOjpaBw8eVPny5W1cLQqT9N6bKesZ/FVKeIqKilL79u0VHh6uSpUq2bJUAACsyFbIq8hWADKL0/dgEykfTD/99JP279+vK1eu6JlnnlGTJk0kSSVLllSVKlVUtGhRzZo1S0WKFFFcXJwsFovc3d3tXD0KspT35tatW7VhwwYZhqGXXnpJvr6+acamTDevUqWKfvnlF+tVYQAAsDWyFfIqshWArGChc9iEyWTS6tWr9dxzz2nt2rU6dOiQmjVrpo8//ljx8fGSJD8/P924cUP/+9//JElFixYlNCHXmUwmffvtt+rUqZOOHj2q7du3q2nTplq7dm2641PCEotvAgDsiWyFvIpsBSAraErBJo4ePaqhQ4dq0qRJ2rJlixYvXizpz6nlKWsauLm56ebNmzp9+rQdK0Vhc+vWLR0/flwzZ87U5s2bFR4erqFDh6pHjx768ssvM9yPRTgBAPZEtkJeRbYCkBU0pWATsbGxqlu3rgYOHKiTJ0+qTp06GjRokMaPHy9JunjxoooUKaLnnntO9evXt2+xKDQOHz6sMmXKaNmyZfLy8pL05+kOkydP1tChQ9WrVy+tXr3azlUCAJAW2Qp5EdkKQFbRlEKOsVgsqf5Xku7evStJiomJ0dmzZ/Xrr7/qySef1NNPP625c+dKkjZu3Kjhw4fr7t27mj9/vmrWrGn74lEoeXp6qnv37jp06JBu3bol6c/3r7OzsyZPnqzg4GB169ZNX331lZ0rBQAURmQr5DdkKwBZRVMKOcZsNuvMmTOaP3++JCksLExPP/207t69q1atWqlixYpq3LixWrZsqU8//dS63/fff69r167pzp07cnV1tVf5KITKli2r999/Xz179tSgQYP0008/yWw2yzAMOTk56d1339U777yjRx55xN6lAgAKIbIV8huyFYCs4vIGyDEWi0UffvihIiIitG/fPi1evFjz58+Xq6urypYtq3bt2unMmTMqWbKkYmJiFBsbq6VLl+qzzz5TRESESpYsae+ngAIs5UowR44c0cWLFxUfH682bdrI09NTn376qZKSkuTv76+tW7eqefPmMgxDzs7OmjBhgr1LBwAUUmQr5GVkKwA5wWQYhmHvIlCwPPXUU9q8ebN69+6t0NBQ63aLxaJ33nlHW7Zs0cGDB1W3bl0lJSUpNDRUDRo0sF/BKDS+/PJLvfrqq6pQoYJ+/vlntWjRQn369NGQIUN0+/ZtDRw4UJs3b9Y333yjxx9/3N7lAgAgiWyFvItsBeBB0ZRCjrl3755MJpN69+6tK1euKCkpSV27dtWQIUPk4uIi6c9vVGJjY3XgwAF5e3urVKlSKlOmjJ0rR0GV8g2eJB08eFBPPvmk3n//fQUGBuru3bt66623dPr0afXu3VuvvPKKrly5ogEDBujQoUM6ceIEpzwAAOyKbIW8hmwFIKfRlMID++uHUwqLxaL+/fvr+PHj6tGjR6rwFBcXp6JFi9qjVBQSy5cvl5+fn8qWLWvdtnTpUk2dOlWRkZEqVqyYTCaTLl68qH/961+6cOGCtmzZImdnZ125ckUJCQkqX768HZ8BAKAwI1shryFbAcgtLHSOB5ISmnbs2KF33nlH8+bN0759+2Q2mzVnzhzVqlVLX3zxhT755BMlJCTonXfeUWBgoJKSkuxdOgqoHTt2aOHChUpISEi13Ww26969e4qPj5fJZFJSUpLKlSuniRMnaseOHdqxY4ckqXTp0oQmAIDdkK2Q15CtAOQmZkrhgX311Vfq2bOnfH19dfXqVRUpUkQTJ07Us88+q9u3b2vEiBGKjIyUYRi6fPmyvvnmGz366KP2LhsFWGxsrDw8PPTzzz/Lw8NDXl5e+uWXX9SgQQONHj1a48ePt449ffq0OnbsqAULFqhp06b2KxoAgP+PbIW8hmwFILdw9T08kMuXL2vfvn2aM2eOBgwYoMjISH322WcKCgqSYRjq2LGjPv74Y23atEmXL19Wu3btVL16dXuXjQIqOTlZDg4O8vDwUHR0tAYOHKjKlStrxowZql27tubNm6dXXnlFycnJeumll1S8eHH997//1c2bN/kGDwCQJ5CtkJeQrQDkNppSyLbDhw+rb9++cnJy0rx58yRJLVq0sJ5THhQUJJPJpGeffVaBgYF2rhYFlcVikdlsVlJSkhwd//yTdvToUdWtW1d9+vTRmjVrNGbMGE2ZMkX9+/eXg4ODgoKCtGTJErm6uiouLk5ff/21vLy87PxMAACFHdkKeQHZCoAtsaYUsi02NlYVKlTQr7/+qlu3blm316tXTyNGjNBTTz2lHj16aPPmzXasEgWd2WzW77//rq5du0qSVq1apccff1zHjh3T0KFD1bVrV508eVJvv/22oqOj1bdvXx08eFDz5s3T9OnTFRkZqUaNGtn5WQAAQLZC3kC2AmBLzJRCtrVr105OTk5KSEjQa6+9poULF1rXM6hbt65ee+01ubi4qEqVKnauFAWdxWLRjh071KhRIx06dEgLFy6Uj4+PJOn111+XJK1YsUKjR4/WpEmTVL16dU51AADkOWQr5BVkKwC2wkLnyJSUK8EcP35ct27d0o0bN9SuXTtJUmRkpN5//32dPXtW8+bNU7Nmzaz7JSQkyNnZ2V5loxCZOXOmgoOD5ePjo3379snV1dU6/VyS5syZoy+//FKlSpXSJ598kuqSxgAA2BrZCnkd2QqALXD6Hu4rJTR9+eWX8vf3V/fu3RUYGKhWrVpp9+7datGihUaOHKlKlSrpjTfe0K5du6z7EppgKzVq1NCkSZN0584dBQQEKCYmRmazWcnJyZL+/FYvMDBQt27dksVisXO1AIDCjGyF/IBsBcAWmCmFTNm9e7c6dOigjz/+WM2bN5ejo6N69OghSfr000/VpEkTff/993rvvfeUnJysjRs3ysXFRSaTyc6Vo6BKCfR///dvv/2m9u3bq0qVKvriiy9UunRpSdLOnTv1xBNP6ObNmypevLjd6gYAQCJbIe8hWwGwB5pSSOWvH0B/vT137lwtWbJEP/zwg5ycnGQ2m3Xv3j01b95cHh4e2rp1q6Q/P5wqV66sChUq2OspoBBIeV9GRkZq//79iomJUf/+/VW1alVJ/xeeKleurClTpujbb7/V/PnzdfjwYa4EAwCwKbIV8gOyFQB7oSkFq5RzxK9cuaIzZ87IZDKpcePGkqRx48Zp1apVOnbsmCTpzp07cnNz08GDB9W2bVtt3rw51XoHQG5JCU1r1qzRK6+8Ih8fH1ksFh05ckTz589XQECA3N3ddfr0aT355JMymUy6e/eu1q5da30/AwBgC2Qr5AdkKwD2xJpSkPR/oenYsWMKDAzUO++8o8mTJyspKUmS9Pzzz+vs2bOaPn26JMnNzU2SlJiYKA8PDz300EP2Kh2FjMlk0q5duzRkyBBNmzZN27dv1/r163Xz5k0FBf2/9u43NOrCD+D4e7dzhjFTywySzpFzOQLXCfbHBZUPROZQyD9jVBaBmgs3aMxpEUSalkVEIshgD2zGcCZ7YmWTWA8m0qCyrTHaxI2h2wgdbDrTbXe/B7L9XD/7EVT3vXPv15ON7z35HBzHm899/5RQV1fHlStXWLBgAe3t7dTU1NDc3Gw0SZISyrZSqrCtJAUpHPQACl48HicUCvHLL7+Qn5/Ptm3b2LJlC/PnzycUChGPx8nOzqaiooKDBw8Sj8cpLy9ncHCQEydOEA6HDSf9q/546cNPP/3E5s2beeWVV+jq6uKZZ56hrKyM69evs337djIyMigoKGD27Nn+yixJSjjbSsnOtpKULLx8TwBcvnyZNWvWEI1G+eSTTyaO3/rY1+7ubmpqati7dy9z587lnnvuobe3l6+++opoNBrU6LpDjX/2xsbGSE9PB6ClpYWsrCx6enoYGRnh4YcfZvXq1SxcuJCqqir6+vrIyclhaGiIzz77jOLiYm8IK0kKhG2lZGNbSUpGniklAPr6+ujt7eX555+fFEvjf+PxOJFIhIqKCtatW8fJkye5//77efzxx8nKygpydN2hQqEQXV1dvPzyyzQ2NlJfX8+2bds4ceIEjz32GABtbW0MDAywadMmAK5evUpRUREzZswgGo0aTZKkwNhWSja2laRk5FJKwM1Tdru7u3n66adJS0ubFE9w81rz4eFhWltbWbZsGTk5OQFOq6lieHiYCxcukJOTQ0dHBzU1NRPRBNDf3097ezvXrl1jYGCAw4cPc/78eb788kvCYb/eJEnBsa2UjGwrScnGG50LgAULFhAOhzl+/DjApGgaV11dzVtvvcWNGzcSPZ6mqNzcXEpLS+no6CASiVBcXAzA2NgYAM8++yxr165l5cqV5Ofn8+mnn7Jv3z6jSZIUONtKyci2kpRsXEoJgEgkwsyZMzl8+DDd3d0Tx2+95VhXVxdLly5l2rRpQYyoKWb8s5ebm8v777/PrFmzyMvLY3BwkPT0dK5fvw5AbW0tR48eZdeuXTQ3N3sPDklSUrCtlGxsK0nJyBuda8Lx48cpLi5mw4YNVFZWkpubC9w8zXf37t18/vnnfPPNNyxatCjgSXUnu/VpMKOjoxO/zLW1tbFhwwbS09M5ffo0d999NwCNjY1Eo1FmzpwZ2MySJN2ObaVkYFtJSmYupTQhFotRVVXF66+/zsKFC3nyySe56667uHDhAmfOnOHrr7+edM259E8bj6aGhgbq6+v59ddfKSwsJD8/n2g0Snt7O+vXrycUClFdXU1dXR21tbU0NTXx4IMPBj2+JEmT2FYKmm0lKdm5lNL/+P7779m/fz+dnZ1kZmby1FNP8eqrr5KdnR30aJoC6uvrKSoqYuPGjYyOjtLU1MQjjzzCa6+9xpo1a+js7OSFF17g4sWLTJs2jaNHj7J06dKgx5Yk6U/ZVgqSbSUpmbmU0m2NjY2Rnp4e9BiaYvr7+ykoKODFF1+ktLQUuBnyH374IUNDQ+zfv59HH3104ngkEmHevHlBjixJ0l9iWykItpWkZOeNznVbtz4hxr2l/ml/9pnKyMhgYGCAOXPmTBxbtmwZ5eXl/PzzzzQ3N086bjRJklKFbaV/k20lKVW5lNJtjd8M8Y//S39XLBYjLS2NS5cu0dbWRktLy8RrQ0NDTJ8+nd9++w2AkZER4GYk5eXlcerUqUBmliTp77Kt9G+xrSSlMpdSkhImFosRCoVobW1l1apVFBQUUFhYyObNmwF46KGHKC4uZufOnXz77beTHpEdj8fJysoKanRJkqSkY1tJSnXhoAeQNDWMR9PZs2dZvnw5W7duZfXq1Rw7doyqqiqWLFlCSUkJlZWV9PT0sHLlSt5++21mzZpFV1cXTU1NfPTRR0G/DUmSpKRgW0m6E7iUkpQQoVCIzs5OnnjiCcrLy3n33XcBiEQiVFVVcf78eQDC4TCHDh0iKyuLL774gt9//525c+fy3XffsXjx4iDfgiRJUtKwrSTdCVxKSUqIWCxGdXU1mZmZ3HvvvRPHa2trGRkZoaOjg48//pj77ruP9evXU1lZyY4dOxgeHiYWi5GZmRng9JIkScnFtpJ0J0iL+/gPSQly8eJFPvjgA86cOcOmTZsYGhpi3759lJSUkJeXx5EjR+jp6aG3t5ecnBzKysooLCwMemxJkqSkZFtJSnUupSQlVF9fH3v27KGhoYFz585x8uRJnnvuOQBGR0cJh8McOHCAH374gfLycnJzcwOeWJIkKXnZVpJSmUspSQnX39/Pe++9R2NjIy+99BJvvPEGADdu3CAjIwP4b0RJkiTp/7OtJKUqv5UkJdy8efPYuXMnsViMuro6RkdH2bFjBxkZGRPBZDRJkiT9NbaVpFTlmVKSAjN+uvmPP/7IihUreOedd4IeSZIkKWXZVpJSTSjoASRNXQ888ABvvvkm2dnZnD59mkuXLgU9kiRJUsqyrSSlGs+UkhS4/v5+4Oap55IkSfp7bCtJqcKllCRJkiRJkhLOy/ckSZIkSZKUcC6lJEmSJEmSlHAupSRJkiRJkpRwLqUkSZIkSZKUcC6lJEmSJEmSlHAupSRJkiRJkpRwLqUkSZIkSZKUcC6lJEmSJEmSlHAupSRJkiRJkpRwLqUkSZIkSZKUcC6lJEmSJEmSlHD/AdmxQtG6c9qSAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4"
      ],
      "metadata": {
        "id": "oHQdeqAXGfRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMggAsYwc-eZ",
        "outputId": "0bc56fcd-5c6e-4cbb-dd3e-6afcf9f0acf0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!cp '/content/drive/MyDrive/DLforVision/TP_SelfSupervised/imagenet.zip' /content/cs294-158-ssl/data/imagenet.zip\n",
        "\n",
        "!unzip -qq /content/cs294-158-ssl/data/imagenet.zip\n",
        "!rm data.zip /content/cs294-158-ssl/data/imagenet.zip\n",
        "\n",
        "#unzipped into /content/cs294-158-ssl istead of /content/cs294-158-ssl/data. Moved manually"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFa1VAa9dAHn",
        "outputId": "d435ae4f-f6f7-4c91-c85c-990829d546c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'data.zip': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from deepul_helper.demos import train_linear_classifier\n",
        "from deepul_helper.demos import evaluate_classifier\n",
        "\n",
        "# Evaluate Context Encoder\n",
        "model, linear_classifier, train_loader, test_loader = train_linear_classifier('context_encoder', 'imagenet', epochs=10)\n",
        "train_acc1_ce, train_acc5_ce = evaluate_classifier(model, linear_classifier, train_loader)\n",
        "test_acc1_ce, test_acc5_ce = evaluate_classifier(model, linear_classifier, test_loader)\n",
        "\n",
        "# Evaluate Rotation Prediction\n",
        "model, linear_classifier, train_loader, test_loader = train_linear_classifier('rotation', 'imagenet', epochs=15)\n",
        "train_acc1_rot, train_acc5_rot = evaluate_classifier(model, linear_classifier, train_loader)\n",
        "test_acc1_rot, test_acc5_rot = evaluate_classifier(model, linear_classifier, test_loader)\n",
        "\n",
        "# Evaluate SimCLR\n",
        "model, linear_classifier, train_loader, test_loader = train_linear_classifier('simclr', 'imagenet', epochs=15)\n",
        "train_acc1_simclr, train_acc5_simclr = evaluate_classifier(model, linear_classifier, train_loader)\n",
        "test_acc1_simclr, test_acc5_simclr = evaluate_classifier(model, linear_classifier, test_loader)\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results = pd.DataFrame({\n",
        "    'Model': ['Context Encoder', 'Rotation Prediction', 'SimCLR'],\n",
        "    'Train Top 1 Accuracy': [train_acc1_ce, train_acc1_rot, train_acc1_simclr],\n",
        "    'Train Top 5 Accuracy': [train_acc5_ce, train_acc5_rot, train_acc5_simclr],\n",
        "    'Test Top 1 Accuracy': [test_acc1_ce, test_acc1_rot, test_acc1_simclr],\n",
        "    'Test Top 5 Accuracy': [test_acc5_ce, test_acc5_rot, test_acc5_simclr]\n",
        "})\n",
        "\n",
        "display(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "afQ7_v1CS_jw",
        "outputId": "70b5efce-784d-47d1-cfbb-65444fc91236"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training new linear classifier for 10 epochs...\n",
            "Epoch 1/10 - Train Loss: 2.558, Train Acc: 22.43%, Test Acc: 23.90%\n",
            "Epoch 2/10 - Train Loss: 2.675, Train Acc: 25.28%, Test Acc: 24.84%\n",
            "Epoch 3/10 - Train Loss: 2.728, Train Acc: 25.51%, Test Acc: 24.79%\n",
            "Epoch 4/10 - Train Loss: 2.761, Train Acc: 25.57%, Test Acc: 25.22%\n",
            "Epoch 5/10 - Train Loss: 2.772, Train Acc: 26.44%, Test Acc: 24.92%\n",
            "Epoch 6/10 - Train Loss: 2.745, Train Acc: 26.38%, Test Acc: 24.46%\n",
            "Epoch 7/10 - Train Loss: 2.693, Train Acc: 27.04%, Test Acc: 25.89%\n",
            "Epoch 8/10 - Train Loss: 2.800, Train Acc: 26.23%, Test Acc: 25.73%\n",
            "Epoch 9/10 - Train Loss: 2.753, Train Acc: 26.60%, Test Acc: 25.91%\n",
            "Epoch 10/10 - Train Loss: 2.743, Train Acc: 26.47%, Test Acc: 25.86%\n",
            "Saved new classifier to results/imagenet_context_encoder/new_linear_classifier.pth\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2653861176.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Evaluate Rotation Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_linear_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rotation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtrain_acc1_rot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc5_rot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtest_acc1_rot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc5_rot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cs294-158-ssl/deepul_helper/demos.py\u001b[0m in \u001b[0;36mtrain_linear_classifier\u001b[0;34m(task, dataset, epochs, lr)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mckpt_pth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'cifar10_{task}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_best.pth.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_pth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'context_encoder'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1551\u001b[0m                 )\n\u001b[1;32m   1552\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         return _legacy_load(\n\u001b[1;32m   1555\u001b[0m             \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8q9WKe0y2bdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate SimCLR\n",
        "model, linear_classifier, train_loader, test_loader = train_linear_classifier('simclr', 'imagenet', epochs=10)\n",
        "train_acc1_simclr, train_acc5_simclr = evaluate_classifier(model, linear_classifier, train_loader)\n",
        "test_acc1_simclr, test_acc5_simclr = evaluate_classifier(model, linear_classifier, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "1LgDYbrm-BO6",
        "outputId": "1eda43a8-05bd-4d6a-8db2-24a3e91ee955"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.optim.lr_scheduler.CosineAnnealingLR was not an allowed global by default. Please use `torch.serialization.add_safe_globals([torch.optim.lr_scheduler.CosineAnnealingLR])` or the `torch.serialization.safe_globals([torch.optim.lr_scheduler.CosineAnnealingLR])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1200657338.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate SimCLR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_linear_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'simclr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_acc1_simclr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc5_simclr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_acc1_simclr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc5_simclr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cs294-158-ssl/deepul_helper/demos.py\u001b[0m in \u001b[0;36mtrain_linear_classifier\u001b[0;34m(task, dataset, epochs, lr)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mckpt_pth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'cifar10_{task}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_best.pth.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_pth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'context_encoder'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1551\u001b[0m                 )\n\u001b[1;32m   1552\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         return _legacy_load(\n\u001b[1;32m   1555\u001b[0m             \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.optim.lr_scheduler.CosineAnnealingLR was not an allowed global by default. Please use `torch.serialization.add_safe_globals([torch.optim.lr_scheduler.CosineAnnealingLR])` or the `torch.serialization.safe_globals([torch.optim.lr_scheduler.CosineAnnealingLR])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ]
    }
  ]
}